{"cells":[{"cell_type":"markdown","id":"b634a117","metadata":{"id":"b634a117"},"source":["## REINFORCE (POLICY-BASED) ALGORITHM ON INVENTORY MANAGEMENT"]},{"cell_type":"markdown","id":"41383775","metadata":{"id":"41383775"},"source":["Suppose a retailer wants to manage the inventory of a retail product in order to maximize product availability over a 3-month period. There are three inventory actions of ORDER, MAINTAIN AND REDUCE. The demand for the product can be High, Medium, or Low.\n"]},{"cell_type":"markdown","id":"1929a6f7","metadata":{"id":"1929a6f7"},"source":["![Product Pricing](https://www.foodrepublic.com/img/gallery/why-you-should-never-eat-canned-food-that-was-accidentally-frozen/l-intro-1693380530.jpg)"]},{"cell_type":"markdown","id":"21a53e20","metadata":{"id":"21a53e20"},"source":["### States:\n","- High\n","\n","- Medium\n","\n","- Low\n","\n","### Actions:\n","- Reduce\n","\n","- Maintain\n","\n","- Order\n","\n"]},{"cell_type":"markdown","id":"18fb7bdf","metadata":{"id":"18fb7bdf"},"source":["### Let's use the REINFORCE Algorithm to Find the Best Policy for the Retail Inventory"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qHQWjGfD__NF","executionInfo":{"status":"ok","timestamp":1731509379329,"user_tz":360,"elapsed":25664,"user":{"displayName":"Bosan Hsu","userId":"02904391632731788520"}},"outputId":"8db05c7f-d098-4098-d465-05bf0cabe1a4"},"id":"qHQWjGfD__NF","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":12,"id":"56ab7f5d","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"id":"56ab7f5d","executionInfo":{"status":"ok","timestamp":1731510108782,"user_tz":360,"elapsed":387,"user":{"displayName":"Bosan Hsu","userId":"02904391632731788520"}},"outputId":"cbea2c42-7592-4027-ae7a-7c30a53f4091"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["     Current Inventory  Demand  Lead Time  Price  Season Action Taken\n","0                    5      91          5     16  Spring       Reduce\n","1                    8      39          5     14  Spring     Maintain\n","2                   10     238         10     20  Winter       Reduce\n","3                   12      37         10     17  Autumn     Maintain\n","4                    4     295          8     13  Summer        Order\n","..                 ...     ...        ...    ...     ...          ...\n","995                 20     229          3     18  Spring     Maintain\n","996                  5      11          8     12  Winter       Reduce\n","997                 23      91          8     11  Winter        Order\n","998                 16      11          9     15  Spring     Maintain\n","999                 23     116          5      1  Spring       Reduce\n","\n","[1000 rows x 6 columns]"],"text/html":["\n","  <div id=\"df-582e8397-8aca-43ab-a88a-131da4cd9fd2\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Current Inventory</th>\n","      <th>Demand</th>\n","      <th>Lead Time</th>\n","      <th>Price</th>\n","      <th>Season</th>\n","      <th>Action Taken</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>5</td>\n","      <td>91</td>\n","      <td>5</td>\n","      <td>16</td>\n","      <td>Spring</td>\n","      <td>Reduce</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>8</td>\n","      <td>39</td>\n","      <td>5</td>\n","      <td>14</td>\n","      <td>Spring</td>\n","      <td>Maintain</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>10</td>\n","      <td>238</td>\n","      <td>10</td>\n","      <td>20</td>\n","      <td>Winter</td>\n","      <td>Reduce</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>12</td>\n","      <td>37</td>\n","      <td>10</td>\n","      <td>17</td>\n","      <td>Autumn</td>\n","      <td>Maintain</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>295</td>\n","      <td>8</td>\n","      <td>13</td>\n","      <td>Summer</td>\n","      <td>Order</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>995</th>\n","      <td>20</td>\n","      <td>229</td>\n","      <td>3</td>\n","      <td>18</td>\n","      <td>Spring</td>\n","      <td>Maintain</td>\n","    </tr>\n","    <tr>\n","      <th>996</th>\n","      <td>5</td>\n","      <td>11</td>\n","      <td>8</td>\n","      <td>12</td>\n","      <td>Winter</td>\n","      <td>Reduce</td>\n","    </tr>\n","    <tr>\n","      <th>997</th>\n","      <td>23</td>\n","      <td>91</td>\n","      <td>8</td>\n","      <td>11</td>\n","      <td>Winter</td>\n","      <td>Order</td>\n","    </tr>\n","    <tr>\n","      <th>998</th>\n","      <td>16</td>\n","      <td>11</td>\n","      <td>9</td>\n","      <td>15</td>\n","      <td>Spring</td>\n","      <td>Maintain</td>\n","    </tr>\n","    <tr>\n","      <th>999</th>\n","      <td>23</td>\n","      <td>116</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>Spring</td>\n","      <td>Reduce</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1000 rows × 6 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-582e8397-8aca-43ab-a88a-131da4cd9fd2')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-582e8397-8aca-43ab-a88a-131da4cd9fd2 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-582e8397-8aca-43ab-a88a-131da4cd9fd2');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-ddf1c14d-65f3-4fd1-8827-ac7834465be6\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ddf1c14d-65f3-4fd1-8827-ac7834465be6')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-ddf1c14d-65f3-4fd1-8827-ac7834465be6 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","  <div id=\"id_f4c1aa05-34f1-49ad-8efb-158518d42f3a\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('inventory_data')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_f4c1aa05-34f1-49ad-8efb-158518d42f3a button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('inventory_data');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"inventory_data","summary":"{\n  \"name\": \"inventory_data\",\n  \"rows\": 1000,\n  \"fields\": [\n    {\n      \"column\": \"Current Inventory\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8,\n        \"min\": 0,\n        \"max\": 30,\n        \"num_unique_values\": 31,\n        \"samples\": [\n          19,\n          17,\n          30\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Demand\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 88,\n        \"min\": 0,\n        \"max\": 300,\n        \"num_unique_values\": 287,\n        \"samples\": [\n          180,\n          21,\n          147\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Lead Time\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 1,\n        \"max\": 10,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          7,\n          10,\n          6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Price\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5,\n        \"min\": 1,\n        \"max\": 20,\n        \"num_unique_values\": 20,\n        \"samples\": [\n          16,\n          15,\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Season\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Winter\",\n          \"Summer\",\n          \"Spring\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Action Taken\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Reduce\",\n          \"Maintain\",\n          \"Order\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":12}],"source":["import numpy as np\n","import pandas as pd\n","\n","# Assume we load the inventory dataset like this (please replace this with actual data)\n","inventory_data = pd.read_csv('/content/drive/MyDrive/Deep Learning/Lab 2 Practice/inventory_dataset.csv')\n","inventory_data"]},{"cell_type":"markdown","id":"740e1804","metadata":{"id":"740e1804"},"source":["#### Q1 Define your states, actions, learning rate (typically between 0.5 - 0.001), and discount factor. Fill in numerical values to replace the question mark (?) for transition_probs and rewards. For transition_probb, ensure that your values for each state-action pair sum up to 1."]},{"cell_type":"code","execution_count":13,"id":"ebb840fc","metadata":{"id":"ebb840fc","executionInfo":{"status":"ok","timestamp":1731510112227,"user_tz":360,"elapsed":322,"user":{"displayName":"Bosan Hsu","userId":"02904391632731788520"}}},"outputs":[],"source":["# Define states, actions, and parameters\n","states =  [\"Low\", \"Medium\", \"High\"]\n","actions = [\"Order\", \"Maintain\", \"Reduce\"]\n","learning_rate =  0.001\n","discount_factor =  0.9\n","\n","# Initialize policy parameters\n","policy_params = {state: {action: 0.0 for action in actions} for state in states}\n","\n","# Define transition probabilities\n","transition_probs = {\n","    (\"Low\", \"Order\"): {\"Low\": 0.8, \"Medium\": 0.2, \"High\": 0.0},\n","    (\"Medium\", \"Order\"): {\"Low\": 0.1, \"Medium\": 0.8, \"High\": 0.1},\n","    (\"High\", \"Order\"): {\"Low\": 0.0, \"Medium\": 0.2, \"High\": 0.8},\n","    (\"Low\", \"Maintain\"): {\"Low\": 0.9, \"Medium\": 0.1, \"High\": 0.0},\n","    (\"Medium\", \"Maintain\"): {\"Low\": 0.0, \"Medium\": 0.9, \"High\": 0.1},\n","    (\"High\", \"Maintain\"): {\"Low\": 0.0, \"Medium\": 0.0, \"High\": 1.0},\n","    (\"Low\", \"Reduce\"): {\"Low\": 0.0, \"Medium\": 0.0, \"High\": 1.0},\n","    (\"Medium\", \"Reduce\"): {\"Low\": 0.0, \"Medium\": 0.0, \"High\": 1.0},\n","    (\"High\", \"Reduce\"): {\"Low\": 0.0, \"Medium\": 0.0, \"High\": 1.0}}\n","\n","# Define rewards\n","rewards = {\n","    'Low': {'Order': 5, 'Maintain': 0.5, 'Reduce': -10},\n","    'Medium': {'Order': 0, 'Maintain': 0.5, 'Reduce': 5},\n","    'High': {'Order': -1, 'Maintain': -0.5, 'Reduce': 2}\n","}"]},{"cell_type":"markdown","id":"515f9d45","metadata":{"id":"515f9d45"},"source":["#### Q2 Implement the Policy Gradient Update Rule in the function, 'update_policy'. Replace the question mark (?)"]},{"cell_type":"code","execution_count":14,"id":"258b3fd7","metadata":{"id":"258b3fd7","executionInfo":{"status":"ok","timestamp":1731510116203,"user_tz":360,"elapsed":556,"user":{"displayName":"Bosan Hsu","userId":"02904391632731788520"}}},"outputs":[],"source":["def get_state_from_inventory(inventory_level):\n","    \"\"\"Convert inventory level to state\"\"\"\n","    if inventory_level <= 10:\n","        return \"Low\"\n","    elif inventory_level <= 20:\n","        return \"Medium\"\n","    else:\n","        return \"High\"\n","\n","def get_policy_probabilities(state):\n","    \"\"\"Convert policy parameters to probabilities using softmax\"\"\"\n","    params = np.array([policy_params[state][action] for action in actions])\n","    exp_params = np.exp(params - np.max(params))\n","    return exp_params / np.sum(exp_params)\n","\n","def choose_action(state):\n","    \"\"\"Choose action based on policy probabilities\"\"\"\n","    probabilities = get_policy_probabilities(state)\n","    return np.random.choice(actions, p=probabilities)\n","\n","def get_next_state(current_state, action):\n","    \"\"\"Determine next state based on transition probabilities\"\"\"\n","    next_state_probs = transition_probs[(current_state, action)]\n","    states_list = list(next_state_probs.keys())\n","    probs = list(next_state_probs.values())\n","    return np.random.choice(states_list, p=probs)\n","\n","def generate_episode():\n","    \"\"\"Generate a single episode\"\"\"\n","    episode = []\n","    current_state = np.random.choice(states)\n","\n","    for _ in range(10):  # Fixed episode length\n","        action = choose_action(current_state)\n","        reward = rewards[current_state][action]\n","        next_state = get_next_state(current_state, action)\n","        episode.append((current_state, action, reward))\n","        current_state = next_state\n","\n","    return episode\n","\n","def calculate_returns(episode):\n","    \"\"\"Calculate returns for each step\"\"\"\n","    returns = []\n","    G = 0\n","    for _, _, reward in reversed(episode):\n","        G = reward + discount_factor * G\n","        returns.insert(0, G)\n","    return returns\n","\n","def update_policy(episode, returns):\n","    \"\"\"Update policy parameters using policy gradient\"\"\"\n","    for (state, action, _), G in zip(episode, returns):\n","        action_probs = get_policy_probabilities(state)\n","        action_idx = actions.index(action)\n","\n","        for a_idx, a in enumerate(actions):\n","            if a_idx == action_idx:\n","                policy_params[state][a] += learning_rate * G * (1 - action_probs[a_idx])\n","            else:\n","                policy_params[state][a] -= learning_rate * G * action_probs[a_idx]\n","\n"]},{"cell_type":"code","execution_count":15,"id":"c8b056a5","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c8b056a5","executionInfo":{"status":"ok","timestamp":1731510121958,"user_tz":360,"elapsed":1383,"user":{"displayName":"Bosan Hsu","userId":"02904391632731788520"}},"outputId":"8bcd98e6-7ba7-4c16-b936-d2a4c846e295"},"outputs":[{"output_type":"stream","name":"stdout","text":["Starting training...\n","Episode 100/1000 completed\n","Episode 200/1000 completed\n","Episode 300/1000 completed\n","Episode 400/1000 completed\n","Episode 500/1000 completed\n","Episode 600/1000 completed\n","Episode 700/1000 completed\n","Episode 800/1000 completed\n","Episode 900/1000 completed\n","Episode 1000/1000 completed\n","\n","Final Policy:\n","\n","State: Low\n","Action: Order, Probability: 0.911\n","Action: Maintain, Probability: 0.066\n","Action: Reduce, Probability: 0.022\n","\n","State: Medium\n","Action: Order, Probability: 0.146\n","Action: Maintain, Probability: 0.178\n","Action: Reduce, Probability: 0.677\n","\n","State: High\n","Action: Order, Probability: 0.025\n","Action: Maintain, Probability: 0.028\n","Action: Reduce, Probability: 0.947\n","\n","Policy Recommendations for Test Inventory Levels:\n","\n","Inventory Level: 5\n","State: Low\n","Recommended Action: Order\n","Action Probabilities:\n","  Order: 0.911\n","  Maintain: 0.066\n","  Reduce: 0.022\n","\n","Inventory Level: 15\n","State: Medium\n","Recommended Action: Reduce\n","Action Probabilities:\n","  Order: 0.146\n","  Maintain: 0.178\n","  Reduce: 0.677\n","\n","Inventory Level: 25\n","State: High\n","Recommended Action: Reduce\n","Action Probabilities:\n","  Order: 0.025\n","  Maintain: 0.028\n","  Reduce: 0.947\n"]}],"source":["# Training loop\n","print(\"Starting training...\")\n","num_episodes = 1000\n","for episode_num in range(num_episodes):\n","    episode = generate_episode()\n","    returns = calculate_returns(episode)\n","    update_policy(episode, returns)\n","\n","    if (episode_num + 1) % 100 == 0:\n","        print(f\"Episode {episode_num + 1}/{num_episodes} completed\")\n","\n","# Print final policy\n","print(\"\\nFinal Policy:\")\n","for state in states:\n","    probs = get_policy_probabilities(state)\n","    print(f\"\\nState: {state}\")\n","    for action, prob in zip(actions, probs):\n","        print(f\"Action: {action}, Probability: {prob:.3f}\")\n","\n","# Test policy on specific inventory levels\n","print(\"\\nPolicy Recommendations for Test Inventory Levels:\")\n","test_inventory_levels = [5, 15, 25]\n","\n","for inventory in test_inventory_levels:\n","    state = get_state_from_inventory(inventory)\n","    probs = get_policy_probabilities(state)\n","    recommended_action = actions[np.argmax(probs)]\n","\n","    print(f\"\\nInventory Level: {inventory}\")\n","    print(f\"State: {state}\")\n","    print(f\"Recommended Action: {recommended_action}\")\n","    print(\"Action Probabilities:\")\n","    for action, prob in zip(actions, probs):\n","        print(f\"  {action}: {prob:.3f}\")"]},{"cell_type":"markdown","id":"81871793","metadata":{"id":"81871793"},"source":["The REINFORCE algorithm has been run for a small number of episodes (10 in this case), and the policy parameters have been updated. The values represent the learned preferences for each action in each state.\n"]},{"cell_type":"markdown","id":"cdd8cc7c","metadata":{"id":"cdd8cc7c"},"source":["Keep in mind that due to the small number of episodes, these values are not necessarily indicative of an optimal policy. In practice, you would run the REINFORCE algorithm for many more episodes and potentially with more sophisticated policy representations (like neural networks) to learn a more reliable policy.​"]},{"cell_type":"markdown","id":"5fba375a","metadata":{"id":"5fba375a"},"source":["## DYNA-Q (MODEL-BASED) ALGORITHM FOR INVENTORY MANAGEMENT"]},{"cell_type":"markdown","id":"a759f9e4","metadata":{"id":"a759f9e4"},"source":["Inventory management is the backbone to any retail business, essentially enabling you to keep your business in order. It’s the system and processes you implement to keep a record of your stores inventory. Inventory management process is crucial."]},{"cell_type":"markdown","id":"baba63d8","metadata":{"id":"baba63d8"},"source":["![Inventory Management](https://aotmp.com/wp-content/uploads/3-Ways-to-Drive-Efficiency-for-Your-Enterprise-Expense-Management-Program-via-Inventory-Management.png)"]},{"cell_type":"markdown","id":"849961a8","metadata":{"id":"849961a8"},"source":["We can define the states, actions, transition probabilities, rewards, and discount factor for an inventory management problem. Let's go through each component:\n","\n","- **States**:\n","    - The states represent the different levels of inventory. In this case, the states are defined as **[\"Low\", \"Medium\", \"High\"]**, indicating low, medium, and high levels of inventory, respectively.\n","\n","- **Actions**:\n","    - The actions represent the decisions the agent can take regarding the inventory. The available actions in this problem are **[\"Order\", \"Maintain\", \"Reduce\"]**, which correspond to ordering more inventory, maintaining the current inventory level, or reducing the inventory level, respectively.\n","\n","    In the context of the product inventory management problem, the actions 'Reduce', 'Maintain', and 'Order' have specific meanings:\n","\n","   - *Reduce*: The 'Reduce' action means **decreasing the product inventory level**. This could involve strategies such as **selling or promoting products** to reduce the inventory to a desired level. The specific implementation of the 'Reduce' action would depend on the business's inventory management practices.\n","\n","   - *Maintain*: The 'Maintain' action means **keeping the product inventory level** unchanged. When the agent selects the 'Maintain' action, it implies that the current inventory level is considered satisfactory, and there is no need to increase or decrease it.\n","\n","   - *Order*: The 'Order' action means **replenishing the product inventory** by placing an order for more products. When the agent chooses the 'Order' action, it indicates that the current inventory level is insufficient, and it is necessary to **order more products** to meet the expected demand.\n","\n","- **Transition Probabilities**:\n","    - The transition probabilities define the likelihood of **moving from one state to another** when a particular action is taken. The probabilities are represented in a nested dictionary format, where the keys are tuples of the form **(current_state, action)**, and the values are dictionaries mapping possible next states to their corresponding probabilities.\n","\n","- **Rewards**:\n","    - The rewards represent the **immediate rewards** associated with transitioning from one state to another after taking a specific action. Similar to transition probabilities, rewards are represented as a nested dictionary, where the keys are tuples of the form **(current_state, action)**, and the values are the associated rewards.\n","\n","- **Discount Factor**:\n","    - The discount factor, represented as discount_factor, **determines the importance of immediate rewards** versus future rewards. It is a value between 0 and 1, where a higher value places more emphasis on future rewards."]},{"cell_type":"markdown","id":"00705a28","metadata":{"id":"00705a28"},"source":["#### Q3 Define your states, actions, transition_probs, rewards, learning rate, discount factor and epsilon."]},{"cell_type":"code","execution_count":16,"id":"515dc17d","metadata":{"id":"515dc17d","executionInfo":{"status":"ok","timestamp":1731510127919,"user_tz":360,"elapsed":288,"user":{"displayName":"Bosan Hsu","userId":"02904391632731788520"}}},"outputs":[],"source":["states = [\"Low\", \"Medium\", \"High\"]"]},{"cell_type":"code","execution_count":17,"id":"aa2a1d37","metadata":{"id":"aa2a1d37","executionInfo":{"status":"ok","timestamp":1731510129185,"user_tz":360,"elapsed":194,"user":{"displayName":"Bosan Hsu","userId":"02904391632731788520"}}},"outputs":[],"source":["actions = [\"Order\", \"Maintain\", \"Reduce\"]"]},{"cell_type":"code","execution_count":18,"id":"f5da4b1c","metadata":{"id":"f5da4b1c","executionInfo":{"status":"ok","timestamp":1731510130201,"user_tz":360,"elapsed":167,"user":{"displayName":"Bosan Hsu","userId":"02904391632731788520"}}},"outputs":[],"source":["transition_probs = {\n","    (\"Low\", \"Order\"): {\"Low\": 0.8, \"Medium\": 0.2, \"High\": 0.0},\n","    (\"Medium\", \"Order\"): {\"Low\": 0.1, \"Medium\": 0.8, \"High\": 0.1},\n","    (\"High\", \"Order\"): {\"Low\": 0.0, \"Medium\": 0.2, \"High\": 0.8},\n","    (\"Low\", \"Maintain\"): {\"Low\": 0.9, \"Medium\": 0.1, \"High\": 0.0},\n","    (\"Medium\", \"Maintain\"): {\"Low\": 0.0, \"Medium\": 0.9, \"High\": 0.1},\n","    (\"High\", \"Maintain\"): {\"Low\": 0.0, \"Medium\": 0.0, \"High\": 1.0},\n","    (\"Low\", \"Reduce\"): {\"Low\": 0.0, \"Medium\": 0.0, \"High\": 1.0},\n","    (\"Medium\", \"Reduce\"): {\"Low\": 0.0, \"Medium\": 0.0, \"High\": 1.0},\n","    (\"High\", \"Reduce\"): {\"Low\": 0.0, \"Medium\": 0.0, \"High\": 1.0}}"]},{"cell_type":"code","execution_count":19,"id":"3bedbfc1","metadata":{"id":"3bedbfc1","executionInfo":{"status":"ok","timestamp":1731510131334,"user_tz":360,"elapsed":5,"user":{"displayName":"Bosan Hsu","userId":"02904391632731788520"}}},"outputs":[],"source":["# Define rewards\n","rewards = {\n","    'Low': {'Order': 5, 'Maintain': 0.5, 'Reduce': -10},\n","    'Medium': {'Order': 0, 'Maintain': 0.5, 'Reduce': 5},\n","    'High': {'Order': -1, 'Maintain': -0.5, 'Reduce': 2}\n","}"]},{"cell_type":"code","execution_count":20,"id":"ca1bfa33","metadata":{"id":"ca1bfa33","executionInfo":{"status":"ok","timestamp":1731510161436,"user_tz":360,"elapsed":119,"user":{"displayName":"Bosan Hsu","userId":"02904391632731788520"}}},"outputs":[],"source":["learning_rate =  0.001\n","discount_factor = 0.9\n","epsilon = 0.1\n","planning_steps = 50  # Number of planning steps for Dyna-Q\n","num_episodes = 1000"]},{"cell_type":"code","execution_count":21,"id":"a0fc22d8","metadata":{"id":"a0fc22d8","executionInfo":{"status":"ok","timestamp":1731510165526,"user_tz":360,"elapsed":291,"user":{"displayName":"Bosan Hsu","userId":"02904391632731788520"}}},"outputs":[],"source":["# Initialize Q-values\n","Q = {\n","    state: {action: 0.0 for action in actions}\n","    for state in states\n","}\n","\n","# Initialize model for Dyna-Q\n","# Model stores (next_state, reward) for each state-action pair\n","model = {\n","    state: {action: [] for action in actions}\n","    for state in states\n","}\n","\n","# Keep track of visited state-action pairs\n","visited_pairs = set()"]},{"cell_type":"markdown","id":"6c7b4834","metadata":{"id":"6c7b4834"},"source":["### Let's use the DYNA-Q Algorithm to obtain the Optimal Q-Values for all State-Action Pairs."]},{"cell_type":"markdown","id":"392d6cf0","metadata":{"id":"392d6cf0"},"source":["We implement the Dyna-Q algorithm below and use the Q-learning update rule to update the Q values:\n","\n","![Alt text](https://miro.medium.com/v2/resize:fit:1400/1*XRF0ejkSFrQsWh55BC1H1Q.png)"]},{"cell_type":"markdown","id":"3efc4f20","metadata":{"id":"3efc4f20"},"source":["#### Q4: Using the Q-Learning Update Rule shown above, write the line for temporal difference target (td_target), TD error (td_error) and Qnew(s,a). td_target, td_error and Q(s,a) has been replaced with question mark (?) in the function, update_q_value."]},{"cell_type":"code","execution_count":22,"id":"523dafbc","metadata":{"id":"523dafbc","executionInfo":{"status":"ok","timestamp":1731510301709,"user_tz":360,"elapsed":143,"user":{"displayName":"Bosan Hsu","userId":"02904391632731788520"}}},"outputs":[],"source":["def get_state_from_inventory(inventory_level):\n","    \"\"\"Convert inventory level to state\"\"\"\n","    if inventory_level <= 10:\n","        return \"Low\"\n","    elif inventory_level <= 20:\n","        return \"Medium\"\n","    else:\n","        return \"High\"\n","\n","def choose_action(state, epsilon=0.1):\n","    \"\"\"Choose action using epsilon-greedy policy\"\"\"\n","    if np.random.random() < epsilon:\n","        return np.random.choice(actions)\n","    else:\n","        return max(actions, key=lambda a: Q[state][a])\n","\n","def get_next_state(current_state, action):\n","    \"\"\"Get next state based on transition probabilities\"\"\"\n","    probs = transition_probs[(current_state, action)]\n","    return np.random.choice(list(probs.keys()), p=list(probs.values()))\n","\n","def get_reward(state, action):\n","    \"\"\"Get reward for state-action pair\"\"\"\n","    return rewards[state][action]\n","\n","def update_q_value(state, action, reward, next_state):\n","    \"\"\"Update Q-value using Q-learning update rule\"\"\"\n","    best_next_action = max(actions, key=lambda a: Q[next_state][a])\n","    td_target = reward + discount_factor * Q[next_state][best_next_action]\n","    td_error = td_target - Q[state][action]\n","    Q[state][action] += learning_rate * td_error\n","\n","def update_model(state, action, next_state, reward):\n","    \"\"\"Update model with observed transition\"\"\"\n","    model[state][action] = (next_state, reward)\n","    visited_pairs.add((state, action))\n","\n","def planning_step():\n","    \"\"\"Perform one planning step using the model\"\"\"\n","    if not visited_pairs:\n","        return\n","\n","    # Randomly select a previously visited state-action pair\n","    state, action = list(visited_pairs)[np.random.randint(len(visited_pairs))]\n","    next_state, reward = model[state][action]\n","\n","    # Update Q-value using the model\n","    update_q_value(state, action, reward, next_state)\n"]},{"cell_type":"code","execution_count":23,"id":"44e50d4d","metadata":{"id":"44e50d4d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1731510311880,"user_tz":360,"elapsed":4354,"user":{"displayName":"Bosan Hsu","userId":"02904391632731788520"}},"outputId":"3db744bc-32f1-444c-af7b-44ca3e24411d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Starting Dyna-Q training...\n","Episode 100/1000 completed\n","Episode 200/1000 completed\n","Episode 300/1000 completed\n","Episode 400/1000 completed\n","Episode 500/1000 completed\n","Episode 600/1000 completed\n","Episode 700/1000 completed\n","Episode 800/1000 completed\n","Episode 900/1000 completed\n","Episode 1000/1000 completed\n"]}],"source":["# Training loop with Dyna-Q\n","print(\"Starting Dyna-Q training...\")\n","for episode in range(num_episodes):\n","    # Start from any state\n","    current_state = np.random.choice(states)\n","\n","    # Real experience\n","    for _ in range(10):  # Steps per episode\n","        # Choose and take action\n","        action = choose_action(current_state, epsilon)\n","        next_state = get_next_state(current_state, action)\n","        reward = get_reward(current_state, action)\n","\n","        # Update Q-value with real experience\n","        update_q_value(current_state, action, reward, next_state)\n","\n","        # Update model\n","        update_model(current_state, action, next_state, reward)\n","\n","        # Planning steps (using the model)\n","        for _ in range(planning_steps):\n","            planning_step()\n","\n","        current_state = next_state\n","\n","    if (episode + 1) % 100 == 0:\n","        print(f\"Episode {episode + 1}/{num_episodes} completed\")\n"]},{"cell_type":"code","execution_count":24,"id":"3c0955ea","metadata":{"id":"3c0955ea","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1731510317886,"user_tz":360,"elapsed":151,"user":{"displayName":"Bosan Hsu","userId":"02904391632731788520"}},"outputId":"c38c3bf3-f0a7-4bd8-c664-55fa06a46457"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Final Q-values:\n","\n","State: Low\n","Action: Order, Q-value: 26.571\n","Action: Maintain, Q-value: 23.480\n","Action: Reduce, Q-value: 7.964\n","\n","State: Medium\n","Action: Order, Q-value: 18.100\n","Action: Maintain, Q-value: 21.162\n","Action: Reduce, Q-value: 22.964\n","\n","State: High\n","Action: Order, Q-value: 17.178\n","Action: Maintain, Q-value: 17.464\n","Action: Reduce, Q-value: 19.965\n","\n","Policy Recommendations for Test Inventory Levels:\n","\n","Inventory Level: 5\n","State: Low\n","Recommended Action: Order\n","Q-values for each action:\n","  Order: 26.571\n","  Maintain: 23.480\n","  Reduce: 7.964\n","\n","Inventory Level: 15\n","State: Medium\n","Recommended Action: Reduce\n","Q-values for each action:\n","  Order: 18.100\n","  Maintain: 21.162\n","  Reduce: 22.964\n","\n","Inventory Level: 25\n","State: High\n","Recommended Action: Reduce\n","Q-values for each action:\n","  Order: 17.178\n","  Maintain: 17.464\n","  Reduce: 19.965\n"]}],"source":["# Print final Q-values\n","print(\"\\nFinal Q-values:\")\n","for state in states:\n","    print(f\"\\nState: {state}\")\n","    for action in actions:\n","        print(f\"Action: {action}, Q-value: {Q[state][action]:.3f}\")\n","\n","# Test policy on specific inventory levels\n","print(\"\\nPolicy Recommendations for Test Inventory Levels:\")\n","test_inventory_levels = [5, 15, 25]\n","\n","for inventory in test_inventory_levels:\n","    state = get_state_from_inventory(inventory)\n","    best_action = max(actions, key=lambda a: Q[state][a])\n","    q_values = [Q[state][a] for a in actions]\n","\n","    print(f\"\\nInventory Level: {inventory}\")\n","    print(f\"State: {state}\")\n","    print(f\"Recommended Action: {best_action}\")\n","    print(\"Q-values for each action:\")\n","    for action, q_value in zip(actions, q_values):\n","        print(f\"  {action}: {q_value:.3f}\")"]},{"cell_type":"markdown","id":"1e087b2f","metadata":{"id":"1e087b2f"},"source":["### Key differences from the REINFORCE implementation:\n","\n","- Q-value Based: Instead of policy parameters, we maintain Q-values for each state-action pair.\n","\n","\n","- Model-Based Learning: The Dyna-Q algorithm maintains a model of the environment that stores observed transitions and rewards.\n","\n","\n","- Planning Steps: After each real experience, the algorithm performs multiple planning steps using the learned model.\n","\n","\n","- Epsilon-Greedy Exploration: Uses an epsilon-greedy policy for action selection instead of probabilistic policy.\n","\n","\n","- While both algorithms can solve reinforcement learning problems, they have distinct characteristics that make them suitable for different scenarios. REINFORCE is better for problems requiring stochastic policies or continuous actions, while Dyna-Q is more efficient for discrete problems where model-based learning can be leveraged.\n","\n","##### Gerald Onwujekwe - PhD"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}