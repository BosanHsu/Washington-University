{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99M5bTbJ8nvQ"
      },
      "source": [
        "# **Lab-based Assignment 5:** Practice - Deep Q Learning for Retail Inventory Management"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sfSNmQb8lLP"
      },
      "source": [
        "![Imgur](https://i.imgur.com/b4dtRYW.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoY3D3WmynCP"
      },
      "source": [
        "## **What is Retail Inventory?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwpSjYQeyp-l"
      },
      "source": [
        "**Retail inventory management** is the backbone to any retail business, essentially enabling you to keep your business in order. It’s the system and processes you implement to keep a record of your stores inventory. Inventory management process is crucial. Having the right **automated inventory management system** in place can make all the difference. Out of stock items equals **frustrated customers and loss of sales** which over time could damage a retailer’s reputation and lose future customers and sales."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnOcqQydyi_y"
      },
      "source": [
        "We can define the states, actions, rewards, and discount factor for an inventory management problem.\n",
        "\n",
        "Let's go through each component:\n",
        "\n",
        "- **States**:\n",
        "    - The states represent the different levels of inventory. In this case, the states are defined as **[\"Low\", \"Medium\", \"High\"]**, indicating low, medium, and high levels of inventory, respectively.\n",
        "\n",
        "- **Actions**:\n",
        "    - The actions represent the decisions the agent can take regarding the inventory. The available actions in this problem are **[\"Order\", \"Maintain\", \"Reduce\"]**, which correspond to ordering more inventory, maintaining the current inventory level, or reducing the inventory level, respectively.\n",
        "\n",
        "    In the context of the product inventory management problem, the actions 'Reduce', 'Maintain', and 'Order' have specific meanings:\n",
        "\n",
        "   - *Reduce*: The 'Reduce' action means **decreasing the product inventory level**. This could involve strategies such as **selling or promoting products** to reduce the inventory to a desired level. The specific implementation of the 'Reduce' action would depend on the business's inventory management practices.\n",
        "\n",
        "   - *Maintain*: The 'Maintain' action means **keeping the product inventory level** unchanged. When the agent selects the 'Maintain' action, it implies that the current inventory level is considered satisfactory, and there is no need to increase or decrease it.\n",
        "\n",
        "   - *Order*: The 'Order' action means **replenishing the product inventory** by placing an order for more products. When the agent chooses the 'Order' action, it indicates that the current inventory level is insufficient, and it is necessary to **order more products** to meet the expected demand.\n",
        "\n",
        "- **Rewards**:\n",
        "    - The rewards represent the **immediate rewards** associated with transitioning from one state to another after taking a specific action. Similar to transition probabilities, rewards are represented as a nested dictionary, where the keys are tuples of the form **(current_state, action)**, and the values are the associated rewards.\n",
        "\n",
        "- **Discount Factor**:\n",
        "    - The discount factor, represented as discount_factor, **determines the importance of immediate rewards** versus future rewards. It is a value between 0 and 1, where a higher value places more emphasis on future rewards."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjmxDsCUysXw"
      },
      "source": [
        "![Imgur](https://i.imgur.com/UUUcCid.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiuKOUgzywPv"
      },
      "source": [
        "We can define the states, actions, rewards, and discount factor for an inventory management problem.\n",
        "\n",
        "Let's go through each component:\n",
        "\n",
        "- **States**:\n",
        "    - The states represent the different levels of inventory. In this case, the states are defined as **[\"Low\", \"Medium\", \"High\"]**, indicating low, medium, and high levels of inventory, respectively.\n",
        "\n",
        "- **Actions**:\n",
        "    - The actions represent the decisions the agent can take regarding the inventory. The available actions in this problem are **[\"Order\", \"Maintain\", \"Reduce\"]**, which correspond to ordering more inventory, maintaining the current inventory level, or reducing the inventory level, respectively.\n",
        "\n",
        "    In the context of the product inventory management problem, the actions 'Reduce', 'Maintain', and 'Order' have specific meanings:\n",
        "\n",
        "   - *Reduce*: The 'Reduce' action means **decreasing the product inventory level**. This could involve strategies such as **selling or promoting products** to reduce the inventory to a desired level. The specific implementation of the 'Reduce' action would depend on the business's inventory management practices.\n",
        "\n",
        "   - *Maintain*: The 'Maintain' action means **keeping the product inventory level** unchanged. When the agent selects the 'Maintain' action, it implies that the current inventory level is considered satisfactory, and there is no need to increase or decrease it.\n",
        "\n",
        "   - *Order*: The 'Order' action means **replenishing the product inventory** by placing an order for more products. When the agent chooses the 'Order' action, it indicates that the current inventory level is insufficient, and it is necessary to **order more products** to meet the expected demand.\n",
        "\n",
        "- **Rewards**:\n",
        "    - The rewards represent the **immediate rewards** associated with transitioning from one state to another after taking a specific action. Similar to transition probabilities, rewards are represented as a nested dictionary, where the keys are tuples of the form **(current_state, action)**, and the values are the associated rewards.\n",
        "\n",
        "- **Discount Factor**:\n",
        "    - The discount factor, represented as discount_factor, **determines the importance of immediate rewards** versus future rewards. It is a value between 0 and 1, where a higher value places more emphasis on future rewards."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPNGXhEFx9AV"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Fs1MF61yy0W"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "# Read the CSV file into a DataFrame\n",
        "df = pd.read_csv('/content/drive/MyDrive/Deep Learning/Lab 5 Files/inventory_dataset.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0LQxQ_8y0j0"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow numpy pandas matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PR5U2unZ52u0"
      },
      "source": [
        "We cover deep learning, experience replay, and Q learning:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AR0ZcYb4NQyS"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "\"\"\"Question 1: Provide the file path\"\"\"\n",
        "# Load the dataset\n",
        "inventory_data = df\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "train_data, test_data = train_test_split(inventory_data, test_size=0.2, random_state=42)  # 80% training, 20% testing\n",
        "\n",
        "# Calculate the average demand from the training set for use in state categorization\n",
        "average_demand = train_data['Demand'].mean()\n",
        "\n",
        "# Function to categorize the state of inventory based on current inventory level and demand\n",
        "def categorize_inventory(inventory, demand):\n",
        "    if demand == 0:\n",
        "        return \"Low Inventory\"\n",
        "    ratio = inventory / average_demand  # Ratio of current inventory to average demand\n",
        "    if ratio < 0.01:\n",
        "        return \"Low Inventory\"\n",
        "    elif ratio <= 0.1:\n",
        "        return \"Medium Inventory\"\n",
        "    else:\n",
        "        return \"High Inventory\"\n",
        "\n",
        "# Define the possible states and actions for the agent\n",
        "states = [\"Low Inventory\", \"Medium Inventory\", \"High Inventory\"]\n",
        "actions = [\"Order\", \"Maintain\", \"Reduce\"]\n",
        "\n",
        "# Define a reward structure for the agent based on state-action pairs\n",
        "rewards = {\n",
        "    ('Low Inventory', 'Order'): 2,\n",
        "    ('Low Inventory', 'Maintain'): -1,\n",
        "    ('Low Inventory', 'Reduce'): -2,\n",
        "    ('Medium Inventory', 'Order'): 1,\n",
        "    ('Medium Inventory', 'Maintain'): 2,\n",
        "    ('Medium Inventory', 'Reduce'): 1,\n",
        "    ('High Inventory', 'Order'): -3,\n",
        "    ('High Inventory', 'Maintain'): -1,\n",
        "    ('High Inventory', 'Reduce'): 2\n",
        "}\n",
        "\n",
        "# One-hot encode states and actions for input into the neural network\n",
        "state_encoding = {state: np.array([i == index for i in range(len(states))]) for index, state in enumerate(states)}\n",
        "action_encoding = {action: np.array([i == index for i in range(len(actions))]) for index, action in enumerate(actions)}\n",
        "\n",
        "#Write the input layer of the model. It is a fully connected (dense) layer with 64 neurons. Set activation to 'relu' and input shape to len(states).\n",
        "#Write the output layer. It is also a fully connected layer with size of len(actions)\n",
        "\n",
        "# Function to build the neural network model for Deep Q-Learning\n",
        "\"\"\"Question 2: Write the input layer of the model. It is a fully connected (dense) layer with 64 neurons.\n",
        "Set activation to 'relu' and input shape to as the len(states).\n",
        "\n",
        "Question 3: Write the output layer. It is also a fully connected (dense) layer with size of len(actions)\"\"\"\n",
        "def build_model():\n",
        "    model = tf.keras.Sequential([\n",
        "        layers.Dense(64, activation='relu', input_shape=(len(states),)),   # Write the input layer of the model here\n",
        "        layers.Dense(64, activation='relu'),  # Hidden layer\n",
        "        layers.Dense(len(actions), activation='linear') # Write the Output layer of the model here\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='mse')  # Compile model with MSE loss function and Adam optimizer\n",
        "    return model\n",
        "\n",
        "# Create the model\n",
        "model = build_model()\n",
        "\n",
        "# Set learning parameters\n",
        "alpha = 0.3  # Learning rate (not used in this snippet, typically for manual updates)\n",
        "initial_epsilon = 1.0  # Starting rate for exploration\n",
        "final_epsilon = 0.01  # Final rate for exploration after decay\n",
        "epsilon_decay = 0.995  # Rate at which to decay exploration\n",
        "epsilon = initial_epsilon  # Initialize epsilon\n",
        "discount_factor = 0.7  # Discount factor for future rewards\n",
        "num_episodes = 20  # Number of episodes to train for\n",
        "buffer_size = 100  # Max size of replay buffer\n",
        "batch_size = 16  # Number of experiences to sample from buffer\n",
        "\n",
        "replay_buffer = deque(maxlen=buffer_size)  # Initialize the replay buffer\n",
        "\n",
        "# Functions to add to and sample from the replay buffer\n",
        "def add_to_buffer(state, action, reward, next_state):\n",
        "    replay_buffer.append((state, action, reward, next_state))  # Add experience to buffer\n",
        "\n",
        "def sample_from_buffer(batch_size):\n",
        "    return random.sample(replay_buffer, min(len(replay_buffer), batch_size))  # Sample a batch from buffer\n",
        "\n",
        "# Lists to track total rewards, losses, epsilon and average Q-values per episode for plotting\n",
        "episode_rewards = []\n",
        "average_q_values = []\n",
        "losses = []\n",
        "epsilon_values = []\n",
        "\n",
        "# Training loop\n",
        "for episode in range(num_episodes):\n",
        "    total_reward = 0\n",
        "    total_q_values = 0\n",
        "    q_value_count = 0\n",
        "    episode_loss = 0\n",
        "\n",
        "    # Iterate over each step in the episode\n",
        "    for index, row in train_data.iterrows():\n",
        "        state = categorize_inventory(row['Current Inventory'], row['Demand'])\n",
        "        state_vector = state_encoding[state]\n",
        "\n",
        "        # Initialize q_values to zeros\n",
        "        q_values = np.zeros((1, len(actions)))\n",
        "\n",
        "        # Decide whether to take a random action or the best action according to the model\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            action = random.choice(actions)\n",
        "        else:\n",
        "            q_values = model.predict(state_vector.reshape(1, -1))\n",
        "            action = actions[np.argmax(q_values[0])]\n",
        "\n",
        "        # Get the next state and reward, and update the replay buffer\n",
        "        next_state = categorize_inventory(row['Current Inventory'], row['Demand'])\n",
        "        next_state_vector = state_encoding[next_state]\n",
        "        reward = rewards[(state, action)]\n",
        "        add_to_buffer(state_vector, action, reward, next_state_vector)\n",
        "\n",
        "        # Update totals\n",
        "        total_reward += reward\n",
        "        if q_values.size > 0:  # Check if q_values has been updated\n",
        "            total_q_values += np.max(q_values[0])\n",
        "        q_value_count += 1\n",
        "\n",
        "    # Training the network on a batch of experiences from the buffer\n",
        "    if len(replay_buffer) >= batch_size:\n",
        "        batch = sample_from_buffer(batch_size)\n",
        "        for exp_state, exp_action, exp_reward, exp_next_state in batch:\n",
        "            # Prepare the target Q-value for training\n",
        "            exp_state = exp_state.reshape(1, -1)\n",
        "            exp_next_state = exp_next_state.reshape(1, -1)\n",
        "            target_q = exp_reward + discount_factor * np.max(model.predict(exp_next_state)[0])\n",
        "            target_q_array = model.predict(exp_state)\n",
        "            action_index = actions.index(exp_action)\n",
        "            target_q_array[0][action_index] = target_q\n",
        "            model.fit(exp_state, target_q_array, epochs=1, verbose=0)\n",
        "            history = model.fit(exp_state, target_q_array, epochs=1,verbose=0)\n",
        "            episode_loss += history.history['loss'][0]\n",
        "\n",
        "    losses.append(episode_loss/len(train_data) if len(train_data) > 0 else 0)\n",
        "\n",
        "    # Decay epsilon and track rewards and Q-values\n",
        "    epsilon = max(final_epsilon, epsilon * epsilon_decay)\n",
        "    episode_rewards.append(total_reward)\n",
        "    average_q_values.append(total_q_values / q_value_count if q_value_count > 0 else 0)\n",
        "    epsilon_values.append(epsilon)\n",
        "\n",
        "# Testing loop to evaluate the performance of the model on unseen data\n",
        "total_reward = 0\n",
        "for index, row in test_data.iterrows():\n",
        "    state = categorize_inventory(row['Current Inventory'], row['Demand'])\n",
        "    state_vector = state_encoding[state].reshape(1, -1)\n",
        "    q_values = model.predict(state_vector)\n",
        "    action = actions[np.argmax(q_values[0])]\n",
        "    next_state = categorize_inventory(row['Current Inventory'], row['Demand'])\n",
        "    reward = rewards[(state, action)]\n",
        "    total_reward += reward\n",
        "\n",
        "\"\"\"Question 4: Write the code to Print total reward from the test data\"\"\"\n",
        "\n",
        "              #Write the code to print total reward here\n",
        "\n",
        "# Plot the rewards and Q-values for analysis\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(episode_rewards)\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Total Reward')\n",
        "plt.title('Total Rewards per Episode')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(average_q_values)\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Average Q-Value')\n",
        "plt.title('Average Q-Values per Episode')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45-L2grqW4_2"
      },
      "outputs": [],
      "source": [
        "# Predict Q-values for each state-action pair\n",
        "state_q_values = {}\n",
        "for state in states:\n",
        "    state_vector = state_encoding[state].reshape(1, -1)\n",
        "    q_values = model.predict(state_vector)\n",
        "    state_q_values[state] = {action: q_values[0][i] for i, action in enumerate(actions)}\n",
        "\n",
        "# Print the Q-values for verification\n",
        "for state, q_vals in state_q_values.items():\n",
        "    print(f\"State: {state}, Q-Values: {q_vals}\")\n",
        "\n",
        "# Create a plot for Q-values\n",
        "plt.figure(figsize=(10, 6))\n",
        "for action in actions:\n",
        "    q_values = [state_q_values[state][action] for state in states]\n",
        "    plt.plot(states, q_values, label=f\"Action: {action}\", marker='o')\n",
        "\n",
        "plt.title(\"Q-Values for Each State-Action Pair\")\n",
        "plt.xlabel(\"State\")\n",
        "plt.ylabel(\"Q-Value\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uPE08Uatb7EU"
      },
      "outputs": [],
      "source": [
        "# Plot Loss Over Episodes\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(losses)\n",
        "plt.title(\"Loss Over Episodes\")\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZ7Oo4hHjavf"
      },
      "source": [
        "The plots depict the total rewards per episode and the average Q-values per episode for a reinforcement learning agent over 100 episodes. Let's analyze each plot:\n",
        "\n",
        "### Total Rewards per Episode:\n",
        "\n",
        "- **Upward Trend**: There's a clear upward trend in the total rewards, indicating that the agent is learning and improving its policy over time.\n",
        "- **Reduced Fluctuations**: Initially, there are some fluctuations, but as episodes increase, the fluctuations in total reward decrease. This suggests that the agent is becoming more consistent in its performance.\n",
        "- **Higher Plateau**: Towards the later episodes, the rewards seem to plateau, suggesting that the agent may be approaching an optimal policy for the environment it is in.\n",
        "\n",
        "### Average Q-Values per Episode:\n",
        "\n",
        "- **Sharp Increase**: The average Q-values rise sharply in the initial episodes, which indicates that the agent is quickly learning from its environment.\n",
        "- **Plateauing of Q-Values**: After the sharp increase, the average Q-values plateau, which typically means the agent has learned to predict the expected rewards from its actions fairly consistently.\n",
        "- **Stability**: The relatively flat line towards the end suggests that the Q-value function has stabilized, which in turn indicates that the agent's policy may have converged.\n",
        "\n",
        "### Combined Interpretation:\n",
        "\n",
        "- **Learning Efficiency**: The agent's learning process seems efficient, as indicated by the consistent upward trend in rewards and average Q values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LY2zGO0AHe_"
      },
      "outputs": [],
      "source": [
        "model.save('/content/drive/MyDrive/Deep Learning/Lab 5 Files/inventory_management_model.keras')\n",
        "\n",
        "print(\"Model saved as 'inventory_management_model.keras'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSU4M8A_-L1H"
      },
      "source": [
        "Using a trained Deep Q-Learning model in a business context, especially for inventory management, involves several steps. These steps include integrating the model into a business process or system, making predictions, and taking actions based on those predictions. Here's a general outline of how you can use your trained model in a business setting:\n",
        "\n",
        "### 1. **Integration with Business Systems**\n",
        "\n",
        "   - **API Development**: Develop an API around your model. This allows various systems in your business to interact with the model. You can use frameworks like Flask or FastAPI for Python to create a simple web service that takes input and returns the model's predictions.\n",
        "   - **Database Integration**: Ensure your model has access to real-time or updated data from your business's inventory database. It's important that the model receives current inventory and demand data to make accurate predictions.\n",
        "\n",
        "### 2. **Making Predictions**\n",
        "\n",
        "   - **Data Preprocessing**: Process the input data (inventory levels, demand, etc.) in the same way as you did during training. This often involves categorizing inventory levels and encoding them before feeding them into the model.\n",
        "   - **Model Prediction**: Use the model to predict the optimal action based on the current inventory state. The model, given a state, will output the Q-values for each action, and you choose the action with the highest Q-value.\n",
        "\n",
        "### 3. **Action Based on Predictions**\n",
        "\n",
        "   - **Implementing Decisions**: The chosen action (e.g., order more stock, maintain current levels, or reduce stock) should be implemented in your inventory management system.\n",
        "   - **Monitoring & Feedback**: Monitor the outcomes of these actions to provide feedback into the system. This can be used for further training or model refinement.\n",
        "\n",
        "### 4. **Model Maintenance and Updating**\n",
        "\n",
        "   - **Regular Re-training**: Update the model periodically with new data to ensure it stays accurate. The business environment often changes, and your model should adapt to these changes.\n",
        "   - **Performance Monitoring**: Continuously monitor the model's performance and its impact on business metrics. If performance degrades, consider retraining the model with more recent data.\n",
        "\n",
        "### 5. **Compliance and Ethical Considerations**\n",
        "\n",
        "   - **Data Privacy**: Ensure that the use of data complies with all relevant data privacy laws and regulations.\n",
        "   - **Transparency**: Maintain transparency about how the model makes decisions, especially if these decisions significantly impact the business or customers.\n",
        "\n",
        "### 6. **User Interface**\n",
        "\n",
        "   - **Dashboard**: Develop a user-friendly dashboard that shows the model's recommendations, current inventory levels, and other relevant metrics. This aids in decision-making and provides insight into the model's performance.\n",
        "\n",
        "### Example Scenario:\n",
        "\n",
        "Imagine a scenario where your model suggests ordering new stock when inventory levels fall into the \"Low Inventory\" category. Your system, via an API, automatically sends this recommendation to the supply chain management team. The team can then review and approve the order, ensuring that decisions are both data-driven and human-reviewed.\n",
        "\n",
        "### Conclusion:\n",
        "\n",
        "Implementing a Deep Q-Learning model in a business, especially in a critical area like inventory management, requires careful planning, continuous monitoring, and regular updates. The key is to ensure that the model's predictions align well with the business's operational realities and objectives."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}