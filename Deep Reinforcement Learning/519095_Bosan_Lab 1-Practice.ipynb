{"cells":[{"cell_type":"markdown","metadata":{"id":"cf98lpdVsr3_"},"source":["# **Lab-based Assignment 1:**"]},{"cell_type":"markdown","metadata":{"id":"gLgVUIbDj3c2"},"source":["![Imgur](https://i.imgur.com/b4dtRYW.png)\n"]},{"cell_type":"markdown","metadata":{"id":"mowy9q6AkhDJ"},"source":["<div class=\"LI-profile-badge\"  data-version=\"v1\" data-size=\"large\" data-locale=\"en_US\" data-type=\"horizontal\" data-theme=\"light\" data-vanity=\"drsalihtutun\"><a class=\"LI-simple-link\" href='https://www.linkedin.com/in/drsalihtutun/en-us?trk=profile-badge'>Salih Tutun, PhD</a></div>"]},{"cell_type":"markdown","metadata":{"id":"Bqy7TcEcaW0v"},"source":["<div class=\"LI-profile-badge\"  data-version=\"v1\" data-size=\"large\" data-locale=\"en_US\" data-type=\"horizontal\" data-theme=\"light\" data-vanity=\"drsalihtutun\"><a class=\"LI-simple-link\" href='https://www.linkedin.com/in/gerald-onwujekwe-ph-d-01984279/'>Gerald Onwujekwe, PhD</a></div>"]},{"cell_type":"markdown","metadata":{"id":"Oh-vhC_QQW6n"},"source":["By the end of this lab, students should be able to:\n","\n","Define and distinguish between Markov Decision Processes (MDP) and Markov Reward Processes (MRP).\n","\n","Understand why most Markov reward and decision processes are discounted.\n","\n","Apply MDPs and MRPs to real-world problems, particularly in retail inventory management."]},{"cell_type":"markdown","metadata":{"id":"q3SU-rzwQW6o"},"source":["# Foundational Knowledge:\n","Basic Python Programming:\n","\n","Fundamental data structures: Lists, dictionaries, and sets.\n","Looping and conditional statements: for and while loops, if-else conditionals.\n","Definition and invocation of functions.\n","\n","Fundamentals of Probability Theory:\n","\n","Definition of events and sample space.\n","Basic properties of probability: such as mutually exclusive events, independent events.\n","Calculation of expectation, variance, and standard deviation.\n","Common probability distributions: e.g., Bernoulli, Normal distribution.\n","\n","Suggested Background Knowledge:\n","\n","Basics of Linear Algebra: Understanding of matrix operations and vector operations, which can aid in computations related to some Markov decision processes.\n","\n","Optimization Techniques: Such as dynamic programming, which is particularly useful when looking for optimal policies.\n","Algorithms and Data Structures: Basic understanding of search algorithms and tree structures, aiding in understanding and implementing decision processes.\n","\n","Tools and Software:\n","Python Environment: It's recommended to use Jupyter Notebook or other Python IDEs such as PyCharm.\n","Relevant Libraries and Frameworks: For instance, numpy (for numerical computations) and matplotlib (for data visualization)."]},{"cell_type":"markdown","metadata":{"id":"seeRnBVBQW6p"},"source":["# Additional Resources:\n","    \n","Book:\n","Reinforcement Learning: An Introduction by Richard S. Sutton and Andrew G. Barto.\n","This is a foundational text for reinforcement learning.\n","http://incompleteideas.net/book/the-book.html\n","\n","Video:\n","David Silver's Reinforcement Learning Course:\n","David Silver, a principal researcher at DeepMind, has a lecture series that's considered one of the best introductions to reinforcement learning\n","https://www.youtube.com/playlist?list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ"]},{"cell_type":"markdown","metadata":{"id":"ih74g6GzQW6p"},"source":["# Q1 Practice Quiz\n","\n","<span style=\"color:red; font-size:22px;\">Fill in the correct answers to the practice quiz.</span>\n","\n","1.What defines a Markov Process?\n","\n","A. The future state depends only on the current state and action.\n","B. The future state depends on all previous states and actions.\n","C. The current action depends on the future state.\n","D. The reward is always deterministic.\n","\n","\n","2.Which of the following is NOT a component of a Markov Decision Process?\n","\n","A. States\n","B. Actions\n","C. Rewards\n","D. Optimizations\n","\n","\n","3.In the context of an MRP, what does the \"reward\" represent?\n","\n","A. The value of the next state.\n","B. The result of an action in a given state.\n","C. The long-term value of a state.\n","D. The transition probability.\n","\n","\n","4.Why are most Markov reward and decision processes discounted?\n","\n","A. To prioritize current rewards over uncertain future rewards.\n","B. To increase the reward over time.\n","C. To decrease computational complexity.\n","D. To ensure all states are visited.\n","\n","\n","Which method is commonly used to find the optimal policy in MDPs?\n","\n","A. Linear Regression\n","B. Dynamic Programming\n","C. Decision Trees\n","D. K-means clustering\n","\n","\n","5.What does the \"Bellman Equation\" provide in the context of MDPs?\n","\n","A. A way to compute transition probabilities.\n","B. A method to calculate immediate rewards.\n","C. A recursive relationship to compute the value of states.\n","D. An algorithm to choose the best action.\n","\n","\n","6.If an MDP has a discount factor (gamma) set to 1, what does it imply?\n","\n","A. Future rewards have no value.\n","B. Immediate rewards are prioritized.\n","C. Future rewards are considered equally as important as current rewards.\n","D. The MDP cannot be solved.\n","\n","\n","7.Which of the following best describes the \"policy\" in an MDP?\n","\n","A. A mapping from states to rewards.\n","B. The set of actions available in each state.\n","C. A mapping from states to optimal actions.\n","D. The transition probability matrix.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0KEeqtt2aW0w"},"outputs":[],"source":["#Type your answers here\n","#1 A\n","#2 D\n","#3 B\n","#4 A\n","#  B\n","#5 B\n","#6 C\n","#7 C"]},{"cell_type":"markdown","metadata":{"id":"cpDyh-NZj6cg"},"source":["# **Markov Decision Process**"]},{"cell_type":"markdown","metadata":{"id":"A3VHQqfm6qz5"},"source":["### Introduction to MDPs"]},{"cell_type":"markdown","metadata":{"id":"-Dvrcqn56aP7"},"source":["- Markov decision processes formally describe an **environment**\n","for reinforcement learning\n","- Almost all RL problems can be formalised as MDPs, e.g.\n","- Partially observable problems can be converted into MDPs\n","    "]},{"cell_type":"markdown","metadata":{"id":"haUQkXGG9Osl"},"source":["### Markov Process\n","\n","A Markov process is a **memoryless random process**, i.e. a sequence\n","of random states S1, S2, ... with the Markov property.\n","\n","Definition:\n","A Markov Process (or Markov Chain) is a tuple 〈S, P〉\n","- S is a (finite) set of states\n","- P is a state transition probability matrix,\n","\n","![Imgur](https://i.imgur.com/AGh6UB1.png)\n"]},{"cell_type":"markdown","metadata":{"id":"KLgMra95975t"},"source":["Example: Student Markov Chain\n","\n","![Imgur](https://i.imgur.com/RyxqIe9.png)\n","\n","Sample episodes for Student Markov\n","Chain starting from S1 = C1\n","\n","S1, S2, ..., ST\n","\n","- C1 C2 C3 Pass Sleep\n","- C1 FB FB C1 C2 Sleep\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"HF95CektaW0y"},"source":["# Q2 Markov Chains\n","<span style=\"color:red; font-size:18px;\">Add 3 more markov chain below for this student starting from C1 and ending in Sleep.</span>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vbFrxdlNaW0y"},"outputs":[],"source":["#Add markov chains here\n","\n","# C1 FB C1 C2 C3 Pub C2 Sleep\n","\n","# C1 FB C1 C2 C3 Pass Sleep\n","\n","# C1 FB C1 C2 C3 Pub C1 C2 Sleep"]},{"cell_type":"markdown","metadata":{"id":"eIweet2U_1eO"},"source":["Student Markov Chain Transition Matrix\n","\n","![Imgur](https://i.imgur.com/QnojYGq.png)"]},{"cell_type":"markdown","metadata":{"id":"hoAvBp6u_61Y"},"source":["# Markov Reward Process\n","\n","A Markov reward process is a **Markov chain with values**.\n","\n","Definition:\n","\n","- A Markov Reward Process is a tuple 〈S, P, R, γ〉\n","\n","- S is a finite set of states\n","\n","- P is a state transition probability matrix,\n","\n","  ![Imgur](https://i.imgur.com/7HmU2AP.png)\n","\n","- R is a reward function, Rs = E [Rt+1 | St = s]\n","\n","- γ is a discount factor, γ ∈ [0, 1]"]},{"cell_type":"markdown","metadata":{"id":"qXSzGaQtCQI6"},"source":["![Imgur](https://i.imgur.com/YoHICOv.png)"]},{"cell_type":"markdown","metadata":{"id":"YSJS7M1FDBh8"},"source":["### Why discount?\n","\n","Most Markov reward and decision processes are discounted. **Why**?\n","- Reflects the agent's time preference, indicating how much the agent values immediate rewards compared to future rewards.\n","- Avoids infinite returns in cyclic Markov processes\n","- Uncertainty about the future may not be fully represented\n","- If the reward is financial, immediate rewards may earn more\n","interest than delayed rewards\n","- It is possible to use undiscounted Markov reward\n","processes (γ=1) where future rewards are equally as important as immediate rewards"]},{"cell_type":"markdown","metadata":{"id":"qW3uvVU-uS9u"},"source":["# **Business Example**"]},{"cell_type":"markdown","metadata":{"id":"7leE2Aj1aqxE"},"source":["**Retail inventory management** is the backbone to any retail business, essentially enabling you to keep your business in order. It’s the system and processes you implement to keep a record of your stores inventory. Inventory management process is crucial. Having the right **automated inventory management system** in place can make all the difference. Out of stock items equals **frustrated customers and loss of sales** which over time could damage a retailer’s reputation and lose future customers and sales."]},{"cell_type":"markdown","metadata":{"id":"2X4_DzWjeZu4"},"source":["![Imgur](https://i.imgur.com/UUUcCid.png)"]},{"cell_type":"markdown","metadata":{"id":"XbPRnjCljjim"},"source":["We can define the states, actions, transition probabilities, rewards, and discount factor for an inventory management problem. Let's go through each component:\n","\n","- **States**:\n","    - The states represent the different levels of inventory. In this case, the states are defined as **[\"Low\", \"Medium\", \"High\"]**, indicating low, medium, and high levels of inventory, respectively.\n","\n","- **Actions**:\n","    - The actions represent the decisions the agent can take regarding the inventory. The available actions in this problem are **[\"Order\", \"Maintain\", \"Reduce\"]**, which correspond to ordering more inventory, maintaining the current inventory level, or reducing the inventory level, respectively.\n","\n","    In the context of the product inventory management problem, the actions 'Reduce', 'Maintain', and 'Order' have specific meanings:\n","\n","   - *Reduce*: The 'Reduce' action means **decreasing the product inventory level**. This could involve strategies such as **selling or promoting products** to reduce the inventory to a desired level. The specific implementation of the 'Reduce' action would depend on the business's inventory management practices.\n","\n","   - *Maintain*: The 'Maintain' action means **keeping the product inventory level** unchanged. When the agent selects the 'Maintain' action, it implies that the current inventory level is considered satisfactory, and there is no need to increase or decrease it.\n","\n","   - *Order*: The 'Order' action means **replenishing the product inventory** by placing an order for more products. When the agent chooses the 'Order' action, it indicates that the current inventory level is insufficient, and it is necessary to **order more products** to meet the expected demand.\n","\n","- **Transition Probabilities**:\n","    - The transition probabilities define the likelihood of **moving from one state to another** when a particular action is taken. The probabilities are represented in a nested dictionary format, where the keys are tuples of the form **(current_state, action)**, and the values are dictionaries mapping possible next states to their corresponding probabilities.\n","\n","- **Rewards**:\n","    - The rewards represent the **immediate rewards** associated with transitioning from one state to another after taking a specific action. Similar to transition probabilities, rewards are represented as a nested dictionary, where the keys are tuples of the form **(current_state, action)**, and the values are the associated rewards.\n","\n","- **Discount Factor**:\n","    - The discount factor, represented as discount_factor, **determines the importance of immediate rewards** versus future rewards. It is a value between 0 and 1, where a higher value places more emphasis on future rewards."]},{"cell_type":"markdown","metadata":{"id":"wTJX4T_mbtKV"},"source":["## MDPs and Value Iteration:"]},{"cell_type":"markdown","metadata":{"id":"ckc7-RIshof9"},"source":["As seen below, the final **value function** represents the **expected profits** associated with each state, and the **optimal policy determines the recommended action for each state to maximize profits.**\n","\n","We start by defining the states, actions, transition probabilities, rewards, and discount factor.\n","\n","Next, we perform value iteration, which involves **iterating over the states and actions** to update the **value function**. We calculate the Q-value for each state-action pair by considering the transition probabilities, rewards, and discounted future values. We update the value function by selecting the maximum Q-value for each state.\n","\n","Finally, we determine the **optimal policy** by selecting the action with the **highest Q-value** for each state. We print the final value function and optimal policy at the end."]},{"cell_type":"markdown","metadata":{"id":"qRiOgAhlElxD"},"source":["# Q3: Mount your Google Drive in Google Colab"]},{"cell_type":"markdown","metadata":{"id":"JjsnEtmQD7VX"},"source":["To mount Google Drive in Colab:\n","\n"," - Run drive.mount('/content/drive').\n","\n"," - Click the provided link to authenticate.\n","\n"," - Sign into Google and grant permissions.\n","\n"," - Copy the given authorization code into the Colab prompt and hit Enter.\n","\n"," - On success, you'll see \"Mounted at /content/drive\".\n","\n","You can now access Drive files with the path /content/drive/MyDrive/. For instance, access my_file.txt at /content/drive/MyDrive/my_file.txt.\n"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"uK_UXM_rmS7n","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1730491705454,"user_tz":300,"elapsed":18759,"user":{"displayName":"Bosan Hsu","userId":"02904391632731788520"}},"outputId":"5c14ad1c-1906-4803-9b5a-97090f0d617b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"vh4Oe1bSEU_Z"},"source":["# Q4：Importing the required libraries:\n","\n","  - The pandas library is imported as pd to work with data structures like DataFrames.\n","\n","Reading the CSV file into a DataFrame:\n","\n","  - The pd.read_csv() function is used to read a CSV file located at the given path (/content/drive/MyDrive/My Drive/Management-MDPsandDynamicProgramming.ipynb) and store its contents in the df DataFrame."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"Myf9oww0E6iz","colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"status":"ok","timestamp":1730491710013,"user_tz":300,"elapsed":1178,"user":{"displayName":"Bosan Hsu","userId":"02904391632731788520"}},"outputId":"1fd2a103-8d35-4913-fc7d-582fdfba09da"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["   Current Inventory  Demand  Lead Time  Price  Season Action Taken\n","0                  5      91          5     16  Spring       Reduce\n","1                  8      39          5     14  Spring     Maintain\n","2                 10     238         10     20  Winter       Reduce\n","3                 12      37         10     17  Autumn     Maintain\n","4                  4     295          8     13  Summer        Order"],"text/html":["\n","  <div id=\"df-0797a048-323b-42fe-ab74-fb5c83dcc9c7\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Current Inventory</th>\n","      <th>Demand</th>\n","      <th>Lead Time</th>\n","      <th>Price</th>\n","      <th>Season</th>\n","      <th>Action Taken</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>5</td>\n","      <td>91</td>\n","      <td>5</td>\n","      <td>16</td>\n","      <td>Spring</td>\n","      <td>Reduce</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>8</td>\n","      <td>39</td>\n","      <td>5</td>\n","      <td>14</td>\n","      <td>Spring</td>\n","      <td>Maintain</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>10</td>\n","      <td>238</td>\n","      <td>10</td>\n","      <td>20</td>\n","      <td>Winter</td>\n","      <td>Reduce</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>12</td>\n","      <td>37</td>\n","      <td>10</td>\n","      <td>17</td>\n","      <td>Autumn</td>\n","      <td>Maintain</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>295</td>\n","      <td>8</td>\n","      <td>13</td>\n","      <td>Summer</td>\n","      <td>Order</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0797a048-323b-42fe-ab74-fb5c83dcc9c7')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-0797a048-323b-42fe-ab74-fb5c83dcc9c7 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-0797a048-323b-42fe-ab74-fb5c83dcc9c7');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-4930de5a-2c44-4599-a1c5-1dfc39c22ffa\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4930de5a-2c44-4599-a1c5-1dfc39c22ffa')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-4930de5a-2c44-4599-a1c5-1dfc39c22ffa button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df","summary":"{\n  \"name\": \"df\",\n  \"rows\": 1000,\n  \"fields\": [\n    {\n      \"column\": \"Current Inventory\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8,\n        \"min\": 0,\n        \"max\": 30,\n        \"num_unique_values\": 31,\n        \"samples\": [\n          19,\n          17,\n          30\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Demand\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 88,\n        \"min\": 0,\n        \"max\": 300,\n        \"num_unique_values\": 287,\n        \"samples\": [\n          180,\n          21,\n          147\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Lead Time\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 1,\n        \"max\": 10,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          7,\n          10,\n          6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Price\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5,\n        \"min\": 1,\n        \"max\": 20,\n        \"num_unique_values\": 20,\n        \"samples\": [\n          16,\n          15,\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Season\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Winter\",\n          \"Summer\",\n          \"Spring\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Action Taken\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Reduce\",\n          \"Maintain\",\n          \"Order\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":2}],"source":["import pandas as pd\n","\n","# Read the CSV file into a DataFrame\n","df = pd.read_csv('/content/drive/MyDrive/Deep Learning/Lab 1 Files/inventory_dataset.csv')\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"fDCJ0bwyE1Y_"},"source":["# Q5: Defining states, actions, transition probabilities, rewards, and the discount factor:\n","\n","  - The states are defined as a list containing three inventory levels: \"Low\", \"Medium\", and \"High\".\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"MimmmAmw61MZ","executionInfo":{"status":"ok","timestamp":1730491711834,"user_tz":300,"elapsed":167,"user":{"displayName":"Bosan Hsu","userId":"02904391632731788520"}}},"outputs":[],"source":["# Step 1: Define the states here\n","states =  [\"Low\", \"Medium\", \"High\"]"]},{"cell_type":"markdown","metadata":{"id":"aYSlDVkGFWjT"},"source":[" - The actions are defined as a list containing three possible actions: \"Order\", \"Maintain\", and \"Reduce\".\n"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"W4Mf3ywRaW0z","executionInfo":{"status":"ok","timestamp":1730491713822,"user_tz":300,"elapsed":152,"user":{"displayName":"Bosan Hsu","userId":"02904391632731788520"}}},"outputs":[],"source":["#Define the actions here\n","actions = [\"Order\", \"Maintain\", \"Reduce\"]"]},{"cell_type":"markdown","metadata":{"id":"4zCkYg-cFYCR"},"source":[" - The transition probabilities are defined using a nested dictionary, where each key is a tuple representing a state-action pair, and the corresponding value is another dictionary mapping states to their respective transition probabilities. Example is {(\"Low\", \"Order\"): {\"Low\": 0.8, \"Medium\": 0.2, \"High\": 0.0}}. Pick the values of your transition probabilities.\n"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"Kpic_fhRFfG8","executionInfo":{"status":"ok","timestamp":1730491715868,"user_tz":300,"elapsed":175,"user":{"displayName":"Bosan Hsu","userId":"02904391632731788520"}}},"outputs":[],"source":["#Define your transition probabilities here\n","transition_probs = {\n","    (\"Low\", \"Order\"): {\"Low\": 0.8, \"Medium\": 0.2, \"High\": 0.0},\n","    (\"Medium\", \"Order\"): {\"Low\": 0.1, \"Medium\": 0.8, \"High\": 0.1},\n","    (\"High\", \"Order\"): {\"Low\": 0.0, \"Medium\": 0.2, \"High\": 0.8},\n","    (\"Low\", \"Maintain\"): {\"Low\": 0.9, \"Medium\": 0.1, \"High\": 0.0},\n","    (\"Medium\", \"Maintain\"): {\"Low\": 0.0, \"Medium\": 0.9, \"High\": 0.1},\n","    (\"High\", \"Maintain\"): {\"Low\": 0.0, \"Medium\": 0.0, \"High\": 1.0},\n","    (\"Low\", \"Reduce\"): {\"Low\": 0.0, \"Medium\": 0.0, \"High\": 1.0},\n","    (\"Medium\", \"Reduce\"): {\"Low\": 0.0, \"Medium\": 0.0, \"High\": 1.0},\n","    (\"High\", \"Reduce\"): {\"Low\": 0.0, \"Medium\": 0.0, \"High\": 1.0}}"]},{"cell_type":"markdown","metadata":{"id":"3k9S2M6_FZN0"},"source":["  - The rewards are defined using a nested dictionary, where each key is a state (\"Low\", \"Medium\" and \"High\"), and the corresponding value is another dictionary mapping actions ('Order','Maintain', 'Reduce') to their respective rewards. Example rewards = {'Low': {'Order': 5, 'Maintain': 0.5, 'Reduce': -10}}. In this example, reward for ordering more product when the inventory is low is 5, reward for maintaining the inventory when the inventory is low is 0.5 and penalty for reducing the inventory when the inventory is low is -10."]},{"cell_type":"code","execution_count":6,"metadata":{"id":"ebZfonlqFgiw","executionInfo":{"status":"ok","timestamp":1730491717611,"user_tz":300,"elapsed":168,"user":{"displayName":"Bosan Hsu","userId":"02904391632731788520"}}},"outputs":[],"source":["#Define the rewards here\n","rewards = {'Low': {'Order': 5, 'Maintain': 0.5, 'Reduce': -10},\n","           'Medium': {'Order': 0.5, 'Maintain': 5, 'Reduce': 0.5},\n","           'High': {'Order': -10, 'Maintain': 0.5, 'Reduce': 5}}\n"]},{"cell_type":"markdown","metadata":{"id":"0s-XiSGKFaWf"},"source":["  - The discount factor is set to 0.9, which determines the importance of future rewards compared to immediate rewards in the decision-making process."]},{"cell_type":"code","execution_count":7,"metadata":{"id":"0GGohYfqFh8o","executionInfo":{"status":"ok","timestamp":1730491719689,"user_tz":300,"elapsed":405,"user":{"displayName":"Bosan Hsu","userId":"02904391632731788520"}}},"outputs":[],"source":["discount_factor = 0.9"]},{"cell_type":"markdown","metadata":{"id":"tl23vvNAnJt3"},"source":["# Value Iteration"]},{"cell_type":"markdown","metadata":{"id":"McP3Sib3xFHq"},"source":["**Find V(s)** and **a = argmax V(s)**"]},{"cell_type":"markdown","metadata":{"id":"5wPhGDBQgbjZ"},"source":["# Q6: Let's code one iteration and see together."]},{"cell_type":"markdown","metadata":{"id":"l0dzRVnvF6gD"},"source":["Step 1: Initializing the value function (V):\n","\n","  - The value function V is initialized as a dictionary with keys representing the different inventory levels (\"Low\", \"Medium\", and \"High\") and initial values set to 0.\n","\n"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"Xlz0sdDPGlaT","executionInfo":{"status":"ok","timestamp":1730491722045,"user_tz":300,"elapsed":225,"user":{"displayName":"Bosan Hsu","userId":"02904391632731788520"}}},"outputs":[],"source":["# Step 1: Initialize the value function arbitrarily\n","\n","V = {\"Low\": 0, \"Medium\": 0, \"High\": 0}"]},{"cell_type":"markdown","metadata":{"id":"-mGNg8PgGZH3"},"source":["Performing iterations of the value iteration algorithm:\n","\n","  - The outer loop runs for a specific number of iterations (in this case, 1 iteration).\n","\n","Initializing the temporary value function (V_prime):\n","\n","  - In each iteration, a new temporary value function V_prime is created as an empty dictionary.\n","\n","Updating the value function:\n","\n","  - The nested loops iterate over each state and action combination, exploring all possible state-action pairs.\n","  - For each state and action, an inner loop iterates over the possible next states.\n","  - The variable transition_prob retrieves the transition probability from the transition_probs dictionary, which represents the probability of transitioning from the current state-action pair to the next state.\n","  - The variable reward retrieves the reward associated with the current state-action pair from the rewards dictionary.\n","  - The value function update step is performed using the Bellman equation, where the value is updated as the sum of the product of the transition probability, the immediate reward, and the discounted value of the next state.\n","  - The value for each action is compared with the maximum value (max_value) encountered so far, and if the current value is greater, it becomes the new maximum value.\n","  - Once the inner loop completes, the maximum value (max_value) is assigned to the temporary value function V_prime for the current state.\n","  - After iterating through all states and actions, the temporary value function V_prime becomes the new value function V for the next iteration.\n","\n","The final value function V represents the optimal value function after the specified number of iterations."]},{"cell_type":"markdown","metadata":{"id":"fVc4QyNKnQHl"},"source":["Update the value function using the **Bellman optimality equation** in Step 2:\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"YPnHCdJYz7k8"},"source":["  ![Imgur](https://i.imgur.com/TyZpMRO.png)\n","\n"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"KzQvHl_ibwsl","executionInfo":{"status":"ok","timestamp":1730491724845,"user_tz":300,"elapsed":221,"user":{"displayName":"Bosan Hsu","userId":"02904391632731788520"}}},"outputs":[],"source":["# Step 2: Perform two iterations of the value iteration algorithm\n","for i in range(1):\n","    V_prime = {}\n","\n","    for state in states: # [\"Low\", \"Medium\", \"High\"] ----\n","        max_value = float('-inf')\n","\n","        for action in actions: #[\"Order\", \"Maintain\", \"Reduce\"] ----\n","            value = 0\n","\n","            for next_state in states: # [\"Low\", \"Medium\", \"High\"] ----loop through each possible next state\n","                transition_prob = transition_probs.get((state, action), {}).get(next_state,0) # T(Low, Order), (Low) = 0.8, T(Low, Order), (Medium) = 0.2, T(Low, Order), (High)\n","                reward = rewards.get(state, {}).get(action, 0) # R(Low, Order) = 5\n","\n","                # Value function update using Bellman equation\n","                value += transition_prob * (reward + discount_factor * V.get(next_state, 0)) # next_state = Low, value = 4, value = 1, value = 0 = 5\n","\n","            if value > max_value:\n","                max_value = value\n","\n","        V_prime[state] = max_value\n","\n","    V = V_prime"]},{"cell_type":"markdown","metadata":{"id":"iCfUhol8HVHV"},"source":["Printing the final value function:\n","\n","  - The code uses a print() statement to display the message \"Value Function after two iterations:\".\n","  - It then iterates over the items in the V dictionary, which contains the final value estimates for each state.\n","  - For each state-value pair, it prints the state and its corresponding value using string formatting.\n","\n","Displaying the values:\n","\n","  - The print() statement within the loop displays the state (state) and its corresponding value (value).\n","  - The f\"V({state}) = {value}\" syntax is used for string formatting, where {state} and {value} are replaced with the actual state and value, respectively."]},{"cell_type":"code","execution_count":10,"metadata":{"id":"BQ31JuRTmyOC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1730491728071,"user_tz":300,"elapsed":180,"user":{"displayName":"Bosan Hsu","userId":"02904391632731788520"}},"outputId":"71c5bdaf-0196-46b7-d541-f43dc347f732"},"outputs":[{"output_type":"stream","name":"stdout","text":["Value Function after two iterations:\n","V(Low) = 5.0\n","V(Medium) = 5.0\n","V(High) = 5.0\n"]}],"source":["# Step 3: Display the final value function after two iterations\n","print(\"Value Function after two iterations:\")\n","for state, value in V.items():\n","    print(f\"V({state}) = {value}\")"]},{"cell_type":"markdown","metadata":{"id":"daJ25daxgR0K"},"source":["# Markov Decision Processes (MDPs)"]},{"cell_type":"markdown","metadata":{"id":"lJaIA85EjB6x"},"source":["**Step 1**: Define the states, actions, transition probabilities, rewards, and discount factor:\n","\n","In this step, we set up the basic elements of our inventory management problem.\n","\n"," - We define the **states**, which represent the different levels of inventory: Low, Medium, and High.\n","\n"," - Actions are the decisions we can make regarding the inventory, such as ordering more products, maintaining the current inventory, or reducing the inventory.\n","\n"," - Transition probabilities describe the likelihood of transitioning from one state to another when we take a specific action.\n","\n"," - Rewards are assigned to each state-action pair to reflect the desirability of a particular action in a specific state.\n","\n"," - Lastly, the discount factor is a value between 0 and 1 that determines the importance of immediate rewards compared to future rewards."]},{"cell_type":"code","execution_count":11,"metadata":{"id":"yE9BhaiL33QL","executionInfo":{"status":"ok","timestamp":1730491730746,"user_tz":300,"elapsed":213,"user":{"displayName":"Bosan Hsu","userId":"02904391632731788520"}}},"outputs":[],"source":["import pandas as pd\n","\n","\n","\n","# Read the CSV file into a DataFrame\n","df = pd.read_csv('/content/drive/MyDrive/Deep Learning/Lab 1 Files/inventory_dataset.csv')\n","\n","# Step 1: Define the states, actions, transition probabilities, rewards, and discount factor\n","states = [\"Low\", \"Medium\", \"High\"]\n","\n","actions = [\"Order\", \"Maintain\", \"Reduce\"]\n","\n","# We can get the transition prob from the previous examples (dataset) as well\n","transition_probs = {\n","    (\"Low\", \"Order\"): {\"Low\": 0.8, \"Medium\": 0.2, \"High\": 0.0},\n","    (\"Medium\", \"Order\"): {\"Low\": 0.1, \"Medium\": 0.8, \"High\": 0.1},\n","    (\"High\", \"Order\"): {\"Low\": 0.0, \"Medium\": 0.2, \"High\": 0.8},\n","    (\"Low\", \"Maintain\"): {\"Low\": 0.9, \"Medium\": 0.1, \"High\": 0.0},\n","    (\"Medium\", \"Maintain\"): {\"Low\": 0.0, \"Medium\": 0.9, \"High\": 0.1},\n","    (\"High\", \"Maintain\"): {\"Low\": 0.0, \"Medium\": 0.0, \"High\": 1.0},\n","    (\"Low\", \"Reduce\"): {\"Low\": 0.0, \"Medium\": 0.0, \"High\": 1.0},\n","    (\"Medium\", \"Reduce\"): {\"Low\": 0.0, \"Medium\": 0.0, \"High\": 1.0},\n","    (\"High\", \"Reduce\"): {\"Low\": 0.0, \"Medium\": 0.0, \"High\": 1.0}}\n","\n","rewards = {'Low': {'Order': 5, 'Maintain': 0.5, 'Reduce': -10},\n","           'Medium': {'Order': 0.5, 'Maintain': 5, 'Reduce': 0.5},\n","           'High': {'Order': -10, 'Maintain': 0.5, 'Reduce': 5}}\n","\n","'''\n","rewards = {\n","    ('Low', 'Order'): 5,         # Reward for ordering more product when the inventory is already low\n","    ('Medium', 'Order'): 0,      # 0 for ordering more product when the inventory is already medium\n","    ('High', 'Order'): -1,       # Penalty for ordering more product when the inventory is already high\n","    ('Low', 'Maintain'): 0.5,    # Slight reward for maintaining the current inventory when it is already low\n","    ('Medium', 'Maintain'): 0.5, # Slight reward for maintaining the current inventory when it is already medium\n","    ('High', 'Maintain'): -0.5,  # Slight penalty for maintaining the current inventory when it is already high\n","    ('Low', 'Reduce'): -10,      # Higher penalty for reducing the inventory when it is low\n","    ('Medium', 'Reduce'): 5,     # Lower reward for reducing the inventory when it is medium\n","    ('High', 'Reduce'): 2        # Lower reward for reducing the inventory when it is high\n","}\n","'''\n","\n","discount_factor = 0.9\n","\n"]},{"cell_type":"markdown","metadata":{"id":"3JWURPLVqlvb"},"source":["# Q7: Calculate the transtional probabilities with the collected dataset..."]},{"cell_type":"markdown","metadata":{"id":"jfzVPDI1IQN2"},"source":["Defining the threshold values for each feature:\n","\n","  - Four dictionaries (inventory_thresholds, demand_thresholds, lead_time_thresholds, price_thresholds) are defined to map feature values to states based on predefined thresholds. For example, inventory_thresholds = {'Low': (0, 5), 'Medium': (5, 10), 'High': (10, float('inf'))}. This means that for our inventory, the lower threshold for the state, 'Low' is 0 and the upper threshold for the state, 'Low' is 5. For the state, 'Medium', the lower and upper thresholds are 5 and 10 and for the state, 'High' the lower and upper threshold are 10 and positive infinity.\n","  - Each dictionary maps a state to a tuple of lower and upper threshold values.\n","  - Lead time is the time between placing an order and receiving the ordered items.\n","\n"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"RNh23Hi0I0PX","executionInfo":{"status":"ok","timestamp":1730491735937,"user_tz":300,"elapsed":235,"user":{"displayName":"Bosan Hsu","userId":"02904391632731788520"}}},"outputs":[],"source":["# Define inventory thresholds(already defined)\n","inventory_thresholds = {'Low': (0, 5), 'Medium': (5, 10), 'High': (10, float('inf'))}"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"PghDSVrxaW05","executionInfo":{"status":"ok","timestamp":1730491738293,"user_tz":300,"elapsed":137,"user":{"displayName":"Bosan Hsu","userId":"02904391632731788520"}}},"outputs":[],"source":["#Define demand thresholds\n","demand_thresholds =  { 'Low': (0, 20), 'Medium': (20, 50), 'High': (50, float('inf'))}"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"IPHBktADaW05","executionInfo":{"status":"ok","timestamp":1730491739817,"user_tz":300,"elapsed":214,"user":{"displayName":"Bosan Hsu","userId":"02904391632731788520"}}},"outputs":[],"source":["#Define lead time thresholds\n","lead_time_thresholds = { 'Low': (0, 3), 'Medium': (3, 7), 'High': (7, float('inf'))}"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"SH6cW8DGaW05","executionInfo":{"status":"ok","timestamp":1730491740715,"user_tz":300,"elapsed":190,"user":{"displayName":"Bosan Hsu","userId":"02904391632731788520"}}},"outputs":[],"source":["#Define price thresholds\n","price_thresholds = { 'Low': (0, 100), 'Medium': (100, 200), 'High': (200, float('inf'))}"]},{"cell_type":"markdown","metadata":{"id":"yaL060jKIw07"},"source":["Mapping feature values to states using the defined thresholds:\n","\n","  - The map_to_state() function takes a feature value and a dictionary of thresholds as input.\n","  - It iterates over the thresholds and checks if the feature value falls within the lower and upper thresholds of each state.\n","  - If a match is found, the corresponding state is returned.\n","  - This function is used within the apply() method to map the feature values of the DataFrame columns ('Current Inventory', 'Demand', 'Lead Time', 'Price') to their respective states.\n","  - The resulting states are stored in a new column called 'Combined State' in the DataFrame df.\n","\n"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"4nthgNmDI5uP","executionInfo":{"status":"ok","timestamp":1730491742156,"user_tz":300,"elapsed":161,"user":{"displayName":"Bosan Hsu","userId":"02904391632731788520"}}},"outputs":[],"source":["# Function to map a feature value to a state based on thresholds\n","def map_to_state(value, thresholds):\n","    for state, (lower, upper) in thresholds.items():\n","        if lower <= value <= upper:\n","            return state"]},{"cell_type":"markdown","metadata":{"id":"DcO_Y4FaI3Gd"},"source":["Defining states and actions based on unique values in the DataFrame:\n","\n","  - The unique values from the 'Combined State' column are extracted and assigned to the states variable.\n","  - The unique values from the 'Action Taken' column are extracted and assigned to the actions variable.\n","\n"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"gua41hEeJElj","executionInfo":{"status":"ok","timestamp":1730491743624,"user_tz":300,"elapsed":137,"user":{"displayName":"Bosan Hsu","userId":"02904391632731788520"}}},"outputs":[],"source":["# Calculate the combined state based on feature thresholds\n","df['Combined State'] = df.apply(lambda row: max(map_to_state(row['Current Inventory'], inventory_thresholds),\n","                                                map_to_state(row['Demand'], demand_thresholds),\n","                                                map_to_state(row['Lead Time'], lead_time_thresholds),\n","                                                map_to_state(row['Price'], price_thresholds)), axis=1)\n","\n","# Define the states and actions based on the unique values in the DataFrame\n","states = df['Combined State'].unique()\n","actions = df['Action Taken'].unique()"]},{"cell_type":"markdown","metadata":{"id":"siMfVXmPI-eF"},"source":["Initializing and calculating transition counts:\n","\n","  - The transition_counts dictionary is initialized, where each key is a tuple (s, a) representing a state-action pair, and each value is another dictionary mapping next states to their respective transition counts.\n","  - A loop iterates over the rows of the DataFrame (df) excluding the last row.\n","  - For each row, the current state, action taken, and next state are extracted.\n","  - The corresponding transition count in the transition_counts dictionary is incremented by 1.\n","\n"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"Q4Y-6qvVJbPU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1730491745391,"user_tz":300,"elapsed":119,"user":{"displayName":"Bosan Hsu","userId":"02904391632731788520"}},"outputId":"8ff124f3-472b-4829-8c1c-6b3d4b337701"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{('Medium', 'Reduce'): {'Medium': 0, 'Low': 0},\n"," ('Medium', 'Maintain'): {'Medium': 0, 'Low': 0},\n"," ('Medium', 'Order'): {'Medium': 0, 'Low': 0},\n"," ('Low', 'Reduce'): {'Medium': 0, 'Low': 0},\n"," ('Low', 'Maintain'): {'Medium': 0, 'Low': 0},\n"," ('Low', 'Order'): {'Medium': 0, 'Low': 0}}"]},"metadata":{},"execution_count":18}],"source":["# Initialize the transition counts\n","transition_counts = {(s, a): {s_prime: 0 for s_prime in states} for s in states for a in actions}\n","transition_counts"]},{"cell_type":"markdown","metadata":{"id":"riKxCsoaI_xd"},"source":["Calculating transition probabilities:\n","\n","  - The transition_probs dictionary is initialized.\n","  - Another loop iterates over the keys and values of the transition_counts dictionary.\n","  - For each state-action pair, the transition probabilities for next states are calculated by dividing the transition count by the sum of all counts for that state-action pair.\n","  - The resulting transition probabilities are stored in the transition_probs dictionary.\n","\n","The transition_probs dictionary represents the calculated transition probabilities based on the provided data and the defined thresholds. It can be used for further analysis and reinforcement learning algorithms."]},{"cell_type":"code","execution_count":19,"metadata":{"id":"Q3yFR-WaC4GI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1730491748418,"user_tz":300,"elapsed":170,"user":{"displayName":"Bosan Hsu","userId":"02904391632731788520"}},"outputId":"5a4c4f9d-d82a-4e19-fa82-31a28d04eb0a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{('Medium', 'Reduce'): {'Medium': 0.5518867924528302,\n","  'Low': 0.4481132075471698},\n"," ('Medium', 'Maintain'): {'Medium': 0.5414364640883977,\n","  'Low': 0.4585635359116022},\n"," ('Medium', 'Order'): {'Medium': 0.5371428571428571,\n","  'Low': 0.46285714285714286},\n"," ('Low', 'Reduce'): {'Medium': 0.6418918918918919, 'Low': 0.3581081081081081},\n"," ('Low', 'Maintain'): {'Medium': 0.6040268456375839,\n","  'Low': 0.3959731543624161},\n"," ('Low', 'Order'): {'Medium': 0.5522388059701493, 'Low': 0.44776119402985076}}"]},"metadata":{},"execution_count":19}],"source":["# Calculate the transition counts\n","for i in range(len(df) - 1):\n","    current_state = df.loc[i, 'Combined State']\n","    action_taken = df.loc[i, 'Action Taken']\n","    next_state = df.loc[i+1, 'Combined State']\n","\n","    transition_counts[(current_state, action_taken)][next_state] += 1\n","\n","# Calculate the transition probabilities\n","transition_probs = {(s, a): {s_prime: count / sum(counts.values()) for s_prime, count in counts.items()} for (s, a), counts in transition_counts.items()}\n","transition_probs"]},{"cell_type":"markdown","metadata":{"id":"6rZaHzW9QW7A"},"source":["# Summary and Takeaways\n","In this lab, we delved deep into the concepts of MDPs and MRPs, understanding their importance in decision-making processes.\n","Through the retail inventory management example, we saw the practical applications of these concepts.\n","In future labs and in the real-world, these concepts can be pivotal for designing algorithms in fields like robotics, finance, and more."]},{"cell_type":"markdown","metadata":{"id":"szcrQXJwFvHE"},"source":["Developed by Salih (salihtutun@wustl.edu), updated by Gerald (geraldo@wustl.edu)."]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":0}