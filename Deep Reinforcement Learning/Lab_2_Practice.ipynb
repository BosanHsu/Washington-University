{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf98lpdVsr3_"
      },
      "source": [
        "# **Lab-based Assignment 2:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLgVUIbDj3c2"
      },
      "source": [
        "![Imgur](https://i.imgur.com/b4dtRYW.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFZG5F7AuXM3"
      },
      "source": [
        "<div class=\"LI-profile-badge\"  data-version=\"v1\" data-size=\"large\" data-locale=\"en_US\" data-type=\"horizontal\" data-theme=\"light\" data-vanity=\"drsalihtutun\"><a class=\"LI-simple-link\" href='https://www.linkedin.com/in/drsalihtutun/en-us?trk=profile-badge'>Salih Tutun, PhD</a></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eT6VIcDuQadi"
      },
      "source": [
        "# Additional Resources:\n",
        "\n",
        "Video:\n",
        "\n",
        "Provides a visual and intuitive understanding of many RL concepts.\n",
        "https://www.youtube.com/playlist?list=PLZbbT5o_s2xoWNVdDudn51XM8lOuZ_Njv\n",
        "\n",
        "Exercises and Problem Sets:\n",
        "\n",
        "http://rail.eecs.berkeley.edu/deeprlcourse/\n",
        "Their assignments and problem sets give practical exercises on the concepts, perfect for students who want to challenge themselves further."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrCyRaT-hbmD"
      },
      "source": [
        "The following codes have appeared in lab1 and will not be described in detail. These codes are **step 1** of MDP, so they are repeated here to facilitate the running of the lab2 code.\n",
        "\n",
        "As we delve into more advanced topics, it's essential to understand that some foundational code from Lab 1 will reappear in Lab 2. The repetition ensures continuity and a deeper understanding of how these blocks work in various algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQkBMY2pQadi"
      },
      "source": [
        "This lab is a direct continuation of Lab 1. If you'd like a refresher on the previous topics, you can review Lab 1.\n",
        "(Add the final link here.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhoaxJNQQadi"
      },
      "source": [
        "# Self-Assessment Quiz for Lab 2\n",
        "1.Which algorithm specifically uses two distinct phases: policy evaluation and policy improvement?\n",
        "- A. Value Iteration\n",
        "- B. Q-learning\n",
        "- C. Monte Carlo Methods\n",
        "- D. Policy Iteration\n",
        "Answer: D\n",
        "\n",
        "2.In the context of the Bellman Equation, what does the term V(s) represent?\n",
        "- A. Value of a specific action\n",
        "- B. Value of the optimal policy\n",
        "- C. Value of a specific state\n",
        "- D. Value of the reward\n",
        "Answer: C\n",
        "\n",
        "3.What role does the discount factor\n",
        "- γ play in the Bellman Equation?\n",
        "- A. It adjusts the immediate reward.\n",
        "- B. It adjusts the value of future rewards.\n",
        "- C. It represents the probability of taking an action.\n",
        "- D. It signifies the convergence threshold.\n",
        "Answer: B\n",
        "\n",
        "4.During the policy improvement step of Policy Iteration, what are we trying to determine?\n",
        "- A. The optimal value function\n",
        "- B. The immediate reward for each state\n",
        "- C. The best action to take from each state given the current value function\n",
        "- D. The convergence rate of the algorithm\n",
        "Answer: C\n",
        "\n",
        "5.In Value Iteration, when do we typically stop iterating?\n",
        "- A. When the policy stops changing\n",
        "- B. When the value function stops changing significantly\n",
        "- C. After a fixed number of iterations\n",
        "- D. When all rewards have been received\n",
        "Answer: B\n",
        "\n",
        "6.Which equation expresses the expected return for a state, considering future states and rewards?\n",
        "- A. Q-function\n",
        "- B. Policy function\n",
        "- C. Reward function\n",
        "- D. Bellman Equation\n",
        "Answer: D\n",
        "\n",
        "7.Which of the following best describes the process of Policy Evaluation in Policy Iteration?\n",
        "- A. Determining the best policy without considering value functions\n",
        "- B. Iteratively updating the value function based on the current policy until it converges\n",
        "- C. Updating the policy based on the maximum expected return for each state\n",
        "- D. Assigning random values to each state and action pair\n",
        "Answer: B\n",
        "\n",
        "8.If the discount factor γ is set to 0, what impact does this have on the decision-making process?\n",
        "- A. The agent will only consider immediate rewards.\n",
        "- B. The agent will give equal importance to both immediate and future rewards.\n",
        "- C. The agent will focus on maximizing long-term rewards.\n",
        "- D. The agent will not take any actions.\n",
        "Answer: A"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Retail Inventory Management -  MDPs and Dynamic Programming**"
      ],
      "metadata": {
        "id": "nlTi5VxIEqMY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Imgur](https://i.imgur.com/jmmupc7.png)\n",
        "\n",
        "By Smart Retail Solutions"
      ],
      "metadata": {
        "id": "9Zau2-YqEt2h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Retail inventory management** is the backbone to any retail business, essentially enabling you to keep your business in order. It’s the system and processes you implement to keep a record of your stores inventory. Inventory management process is crucial. Having the right **automated inventory management system** in place can make all the difference. Out of stock items equals **frustrated customers and loss of sales** which over time could damage a retailer’s reputation and lose future customers and sales."
      ],
      "metadata": {
        "id": "OA7YPgw7EwbA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Imgur](https://i.imgur.com/UUUcCid.png)"
      ],
      "metadata": {
        "id": "pD1PH8ZIEzKy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can define the states, actions, transition probabilities, rewards, and discount factor for an inventory management problem. Let's go through each component:\n",
        "\n",
        "- **States**:\n",
        "    - The states represent the different levels of inventory. In this case, the states are defined as **[\"Low\", \"Medium\", \"High\"]**, indicating low, medium, and high levels of inventory, respectively.\n",
        "\n",
        "- **Actions**:\n",
        "    - The actions represent the decisions the agent can take regarding the inventory. The available actions in this problem are **[\"Order\", \"Maintain\", \"Reduce\"]**, which correspond to ordering more inventory, maintaining the current inventory level, or reducing the inventory level, respectively.\n",
        "\n",
        "    In the context of the product inventory management problem, the actions 'Reduce', 'Maintain', and 'Order' have specific meanings:\n",
        "\n",
        "   - *Reduce*: The 'Reduce' action means **decreasing the product inventory level**. This could involve strategies such as **selling or promoting products** to reduce the inventory to a desired level. The specific implementation of the 'Reduce' action would depend on the business's inventory management practices.\n",
        "\n",
        "   - *Maintain*: The 'Maintain' action means **keeping the product inventory level** unchanged. When the agent selects the 'Maintain' action, it implies that the current inventory level is considered satisfactory, and there is no need to increase or decrease it.\n",
        "\n",
        "   - *Order*: The 'Order' action means **replenishing the product inventory** by placing an order for more products. When the agent chooses the 'Order' action, it indicates that the current inventory level is insufficient, and it is necessary to **order more products** to meet the expected demand.\n",
        "\n",
        "- **Transition Probabilities**:\n",
        "    - The transition probabilities define the likelihood of **moving from one state to another** when a particular action is taken. The probabilities are represented in a nested dictionary format, where the keys are tuples of the form **(current_state, action)**, and the values are dictionaries mapping possible next states to their corresponding probabilities.\n",
        "\n",
        "- **Rewards**:\n",
        "    - The rewards represent the **immediate rewards** associated with transitioning from one state to another after taking a specific action. Similar to transition probabilities, rewards are represented as a nested dictionary, where the keys are tuples of the form **(current_state, action)**, and the values are the associated rewards.\n",
        "\n",
        "- **Discount Factor**:\n",
        "    - The discount factor, represented as discount_factor, **determines the importance of immediate rewards** versus future rewards. It is a value between 0 and 1, where a higher value places more emphasis on future rewards."
      ],
      "metadata": {
        "id": "zW-I54DGE2Vd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Imgur](https://i.imgur.com/vAJdHgY.png)\n",
        "\n",
        "By VentureBeat"
      ],
      "metadata": {
        "id": "_dVcChO81qAG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "GqiGESl8o1bo",
        "outputId": "64034c31-37ce-4330-b3d3-9d0f83774583",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q1: Let's define the states, actions, discount factor, and rewards?"
      ],
      "metadata": {
        "id": "v9SJdqqeXTqU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1:** Define the states, actions, rewards, etc."
      ],
      "metadata": {
        "id": "JmaUtfMgY9en"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "yE9BhaiL33QL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file into a DataFrame\n",
        "df = pd.read_csv('/content/inventory_dataset.csv')\n",
        "\n",
        "# Step 1: Define the states, actions, transition probabilities, rewards, and discount factor\n",
        "states = [\"Low\", \"Medium\", \"High\"]\n",
        "\n",
        "actions = [\"Order\", \"Maintain\", \"Reduce\"]\n",
        "\n",
        "# We can get the transition prob from the previous examples (dataset) as well\n",
        "transition_probs = {\n",
        "   (\"Low\", \"Order\"): {\"Low\": 0.8, \"Medium\": 0.2, \"High\": 0.0},\n",
        "   (\"Medium\", \"Order\"): {\"Low\": 0.1, \"Medium\": 0.8, \"High\": 0.1},\n",
        "   (\"High\", \"Order\"): {\"Low\": 0.0, \"Medium\": 0.2, \"High\": 0.8},\n",
        "   (\"Low\", \"Maintain\"): {\"Low\": 0.9, \"Medium\": 0.1, \"High\": 0.0},\n",
        "   (\"Medium\", \"Maintain\"): {\"Low\": 0.0, \"Medium\": 0.9, \"High\": 0.1},\n",
        "   (\"High\", \"Maintain\"): {\"Low\": 0.0, \"Medium\": 0.0, \"High\": 1.0},\n",
        "   (\"Low\", \"Reduce\"): {\"Low\": 0.0, \"Medium\": 0.0, \"High\": 1.0},\n",
        "   (\"Medium\", \"Reduce\"): {\"Low\": 0.0, \"Medium\": 0.0, \"High\": 1.0},\n",
        "   (\"High\", \"Reduce\"): {\"Low\": 0.0, \"Medium\": 0.0, \"High\": 1.0}\n",
        "}\n",
        "\n",
        "rewards = {\n",
        "    ('Low', 'Order'): 5,         # Reward for ordering more product when the inventory is already low\n",
        "    ('Medium', 'Order'): 0,      # 0 for ordering more product when the inventory is already medium\n",
        "    ('High', 'Order'): -1,       # Penalty for ordering more product when the inventory is already high\n",
        "    ('Low', 'Maintain'): 0.5,    # Slight reward for maintaining the current inventory when it is already low\n",
        "    ('Medium', 'Maintain'): 0.5, # Slight reward for maintaining the current inventory when it is already medium\n",
        "    ('High', 'Maintain'): -0.5,  # Slight penalty for maintaining the current inventory when it is already high\n",
        "    ('Low', 'Reduce'): -10,      # Higher penalty for reducing the inventory when it is low\n",
        "    ('Medium', 'Reduce'): 5,     # Lower reward for reducing the inventory when it is medium\n",
        "    ('High', 'Reduce'): 2        # Lower reward for reducing the inventory when it is high\n",
        "}\n",
        "\n",
        "discount_factor = 0.9\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q2: Could you define the thresholds and map the states as low, medium, and high from the dataset? Please look at the dataset and decide this!"
      ],
      "metadata": {
        "id": "UPTLTEHRXjNp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "RNh23Hi0I0PX"
      },
      "outputs": [],
      "source": [
        "# Define the threshold values for each feature\n",
        "inventory_thresholds = {'Low': (0, 5), 'Medium': (5, 10), 'High': (10, float('inf'))}\n",
        "demand_thresholds = {'Low': (-float('inf'), 5), 'Medium': (5, 8), 'High': (8, float('inf'))}\n",
        "lead_time_thresholds = {'Low': (1, 2), 'Medium': (2, 3), 'High': (3, float('inf'))}\n",
        "price_thresholds = {'Low': (-float('inf'), 4), 'Medium': (4, 10), 'High': (10, float('inf'))}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "4nthgNmDI5uP"
      },
      "outputs": [],
      "source": [
        "# Function to map a feature value to a state based on thresholds\n",
        "def map_to_state(value, thresholds):\n",
        "    for state, (lower, upper) in thresholds.items():\n",
        "        if lower <= value <= upper:\n",
        "            return state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "gua41hEeJElj"
      },
      "outputs": [],
      "source": [
        "# Calculate the combined state based on feature thresholds\n",
        "df['Combined State'] = df.apply(lambda row: max(map_to_state(row['Current Inventory'], inventory_thresholds),\n",
        "                                                map_to_state(row['Demand'], demand_thresholds),\n",
        "                                                map_to_state(row['Lead Time'], lead_time_thresholds),\n",
        "                                                map_to_state(row['Price'], price_thresholds)), axis=1)\n",
        "\n",
        "# Define the states and actions based on the unique values in the DataFrame\n",
        "states = df['Combined State'].unique()\n",
        "actions = df['Action Taken'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Have some brief information about states\n",
        "df['Combined State']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "HdyGfl1M-IWg",
        "outputId": "b04f308f-7cf8-4bb7-d7d3-642bf580bd6d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0         Low\n",
              "1      Medium\n",
              "2      Medium\n",
              "3        High\n",
              "4         Low\n",
              "        ...  \n",
              "995    Medium\n",
              "996       Low\n",
              "997      High\n",
              "998      High\n",
              "999       Low\n",
              "Name: Combined State, Length: 1000, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Combined State</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Low</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Medium</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Medium</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>High</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Low</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>Medium</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>Low</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>High</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>High</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>Low</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['Action Taken']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "Ay1QdEYPL7RW",
        "outputId": "5b03908d-ac48-48f3-c8ec-dabd635a04a3"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        Reduce\n",
              "1      Maintain\n",
              "2        Reduce\n",
              "3      Maintain\n",
              "4         Order\n",
              "         ...   \n",
              "995    Maintain\n",
              "996      Reduce\n",
              "997       Order\n",
              "998    Maintain\n",
              "999      Reduce\n",
              "Name: Action Taken, Length: 1000, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Action Taken</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Reduce</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Maintain</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Reduce</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Maintain</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Order</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>Maintain</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>Reduce</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>Order</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>Maintain</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>Reduce</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q3: Pleased calculate the transitions probabilities based on the dataset?"
      ],
      "metadata": {
        "id": "SfntGbFDYEZz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To initialize the transition counts, we have created a nested dictionary structure where each state-action pair maps to another dictionary that holds counts of transitions to all possible next states. This structure will facilitate the counting of transitions observed in our dataset.\n",
        "\n",
        "For each state s and action a, we've set up a dictionary that maps to each possible next state s′ with an initial count of 0. As we iterate through our data, we'll increment these counts based on the observed transitions.\n",
        "\n",
        "Here is a representation of what the transition_counts structure looks like:"
      ],
      "metadata": {
        "id": "Bnr6rV2Kx1ln"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![imgur](https://i.imgur.com/9kG2YRk.png)"
      ],
      "metadata": {
        "id": "A43NBk-GxzMs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the transition counts\n",
        "transition_counts = {(s, a): {s_prime: 0 for s_prime in states} for s in states for a in actions}\n",
        "transition_counts"
      ],
      "metadata": {
        "id": "35Ky9H67atzs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64a8c411-f04d-4312-d0eb-8976708045cd"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{('Low', 'Reduce'): {'Low': 0, 'Medium': 0, 'High': 0},\n",
              " ('Low', 'Maintain'): {'Low': 0, 'Medium': 0, 'High': 0},\n",
              " ('Low', 'Order'): {'Low': 0, 'Medium': 0, 'High': 0},\n",
              " ('Medium', 'Reduce'): {'Low': 0, 'Medium': 0, 'High': 0},\n",
              " ('Medium', 'Maintain'): {'Low': 0, 'Medium': 0, 'High': 0},\n",
              " ('Medium', 'Order'): {'Low': 0, 'Medium': 0, 'High': 0},\n",
              " ('High', 'Reduce'): {'Low': 0, 'Medium': 0, 'High': 0},\n",
              " ('High', 'Maintain'): {'Low': 0, 'Medium': 0, 'High': 0},\n",
              " ('High', 'Order'): {'Low': 0, 'Medium': 0, 'High': 0}}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transition Counts:** We loop through the DataFrame containing our data, using df.loc to access the 'Combined State' and 'Action Taken' at each row (the current state and action), and the 'Combined State' of the next row (the next state). For each such triplet (current state, action, next state), we increment the count in the transition_counts dictionary.\n",
        "\n",
        "**Transition Probabilities:** After we've accumulated the counts, we calculate the transition probabilities. For each state-action pair, we divide the count of each observed next state by the total count of all next states that follow from that state-action pair. This results in a probability distribution over next states for each state-action pair."
      ],
      "metadata": {
        "id": "zwaKRoG_xujI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the transition counts\n",
        "for i in range(len(df) - 1):\n",
        "    current_state = df.loc[i, 'Combined State']\n",
        "    action_taken = df.loc[i, 'Action Taken']\n",
        "    next_state = df.loc[i+1, 'Combined State']\n",
        "\n",
        "    transition_counts[(current_state, action_taken)][next_state] += 1\n",
        "\n",
        "# Calculate the transition probabilities\n",
        "transition_probs = {(s, a): {s_prime: count / sum(counts.values()) for s_prime, count in counts.items()} for (s, a), counts in transition_counts.items()}\n",
        "transition_probs"
      ],
      "metadata": {
        "id": "NPVgx8cJa7s_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a9b99e1-9c18-484a-97c8-462f4fd3f973"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{('Low', 'Reduce'): {'Low': 0.371900826446281,\n",
              "  'Medium': 0.4132231404958678,\n",
              "  'High': 0.21487603305785125},\n",
              " ('Low', 'Maintain'): {'Low': 0.30303030303030304,\n",
              "  'Medium': 0.5050505050505051,\n",
              "  'High': 0.1919191919191919},\n",
              " ('Low', 'Order'): {'Low': 0.3275862068965517,\n",
              "  'Medium': 0.4827586206896552,\n",
              "  'High': 0.1896551724137931},\n",
              " ('Medium', 'Reduce'): {'Low': 0.3390804597701149,\n",
              "  'Medium': 0.4367816091954023,\n",
              "  'High': 0.22413793103448276},\n",
              " ('Medium', 'Maintain'): {'Low': 0.31724137931034485,\n",
              "  'Medium': 0.42758620689655175,\n",
              "  'High': 0.25517241379310346},\n",
              " ('Medium', 'Order'): {'Low': 0.26865671641791045,\n",
              "  'Medium': 0.5,\n",
              "  'High': 0.23134328358208955},\n",
              " ('High', 'Reduce'): {'Low': 0.4,\n",
              "  'Medium': 0.4461538461538462,\n",
              "  'High': 0.15384615384615385},\n",
              " ('High', 'Maintain'): {'Low': 0.37209302325581395,\n",
              "  'Medium': 0.4418604651162791,\n",
              "  'High': 0.18604651162790697},\n",
              " ('High', 'Order'): {'Low': 0.4067796610169492,\n",
              "  'Medium': 0.423728813559322,\n",
              "  'High': 0.1694915254237288}}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bPxdYANQadm"
      },
      "source": [
        "## Q4: Perform Value Iteration to Estimate the Optimal Value Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vv7gbLPGgIjL"
      },
      "source": [
        "### Value Iteration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVqMqE1SjIzB"
      },
      "source": [
        "**Step 2**: Initialize the value function with zeros for each state:\n",
        "\n",
        "The value function represents the **expected return** or value of being in a **particular state**. In this step, we start by initializing the value function for each state with zero. This sets a baseline for the expected value of being in each state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "NDkz4e104Gez"
      },
      "outputs": [],
      "source": [
        "# Step 2: Initialize the value function with zeros for each state\n",
        "values = {state: 0 for state in states}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Gpk8RMtjM1s"
      },
      "source": [
        "**Step 3**: Perform value iteration to estimate the optimal value function:\n",
        "\n",
        "Value iteration is a process where we iteratively update the value function until it converges to the optimal values. We repeat this process for a specified number of iterations.\n",
        "\n",
        "Within each iteration, we calculate the V-value for each action in the current state. The V-value represents the expected total discounted reward when taking a specific action in a given state and following the optimal policy thereafter. We calculate the V-value by considering the transition probabilities, rewards, and the discounted future value of the next state. We update the value function for the current state by selecting the maximum V-value among all available actions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWp6ETcAS2Vi"
      },
      "source": [
        "The equation for the expected cumulative reward in the value function can be defined as follows:\n",
        "\n",
        "Expected Cumulative Reward\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMurN87GidLh"
      },
      "source": [
        " ![imgur](https://i.imgur.com/pHPQ7AD.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQTW7GDDifyQ"
      },
      "source": [
        "![imgur](https://i.imgur.com/ptFe5kL.png)\n",
        "\n",
        "The value function V(s) is recursively defined, taking into account the expected cumulative rewards of all possible next states weighted by their transition probabilities and immediate rewards. The goal of value iteration is to iteratively update the value function until it converges to the optimal value function that maximizes the expected cumulative reward for each state."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1Yab7vbQadn"
      },
      "source": [
        "Initialization:\n",
        "Objective: Set the stage for iterative value estimation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "BWLTv5JwQadn"
      },
      "outputs": [],
      "source": [
        "num_iterations = 1000  # Number of iterations for value iteration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zqv1ICxyQadn"
      },
      "source": [
        "Iterative Value Estimation:\n",
        "Objective: For each state, evaluate the maximum expected return over all possible actions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "LnALtyVgQadn"
      },
      "outputs": [],
      "source": [
        "for _ in range(num_iterations):\n",
        "    policy = {}  # Temporary storage for updated state values\n",
        "\n",
        "    for state in states:\n",
        "        V_values = []  # To store values for each action in the current state\n",
        "\n",
        "        for action in actions:\n",
        "            V_value = 0  # Initialize the expected return for current state-action\n",
        "\n",
        "            # Calculate expected return based on possible transitions\n",
        "            for next_state in states:\n",
        "                prob = transition_probs[(state, action)][next_state]  # Transition probability\n",
        "                reward = rewards[(state, action)]  # Immediate reward for action\n",
        "                V_value += prob * (reward + discount_factor * values[next_state])\n",
        "\n",
        "            V_values.append(V_value)  # Collect expected returns for all actions\n",
        "\n",
        "        policy[state] = max(V_values)  # Pick the action with the highest expected return\n",
        "\n",
        "#This nested loop essentially computes the maximum expected return for each state over all possible actions,\n",
        "#considering possible state transitions, immediate rewards, and future rewards."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2Lydx-4Qadn"
      },
      "source": [
        "Update Value Function:\n",
        "Objective: After evaluating values for all states in an iteration, update the main value function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "F9G0CrA6Qadn"
      },
      "outputs": [],
      "source": [
        "    values = policy  # Update the main value function"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q5: Please determine the optimal policy by using the final value function?"
      ],
      "metadata": {
        "id": "pu3k_wbGZIxP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4a7YKzajPn-"
      },
      "source": [
        "**Step 4**: Based on the final value function, determine the optimal policy:\n",
        "\n",
        "Once we have the final value function, we can determine the optimal policy. The policy tells us the best action to take in each state to maximize long-term cumulative rewards. In this step, we calculate the Q-value again for each state and action. We select the action with the highest Q-value as the optimal action for that state. We store the optimal policy in a dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "7r63uT6d4LtI"
      },
      "outputs": [],
      "source": [
        "# Step 4: Based on the final value function, determine the optimal policy\n",
        "policy = {}\n",
        "for state in states:\n",
        "    max_action_idx = V_values.index(max(V_values))  # Find the index of the action with the highest V-value\n",
        "    policy[state] = actions[max_action_idx]  # Determine the optimal action for the current state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ter8HEVjSMN"
      },
      "source": [
        "**Step 5:** Print the final value function and optimal policy:\n",
        "\n",
        "Finally, we print the results of our deep reinforcement learning process. The final value function provides insights into the expected rewards for each state. It tells us how valuable it is to be in a particular state. The optimal policy guides us on the best actions to take in each state to optimize inventory management.\n",
        "\n",
        "By running this code, we get the final value function and the optimal policy printed as output, allowing us to understand the expected rewards and make informed decisions regarding inventory management."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "hOZAtyQN4OAS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f72ef14-2aaf-48c8-8528-a987cc9d2b64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Value Function:\n",
            "{'Low': 4.999999999999999, 'Medium': 5.0, 'High': 2.0}\n",
            "\n",
            "Optimal Policy:\n",
            "{'Low': 'Reduce', 'Medium': 'Reduce', 'High': 'Reduce'}\n"
          ]
        }
      ],
      "source": [
        "# Step 5: Print the final value function and optimal policy\n",
        "print(\"Final Value Function:\")\n",
        "print(values)\n",
        "print(\"\\nOptimal Policy:\")\n",
        "print(policy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6we9FP1DJxfu"
      },
      "source": [
        "# Policy Iteration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLytiNk0t1Tu"
      },
      "source": [
        "**Step 1**: Define the states, actions, transition probabilities, rewards, and discount factor:\n",
        "\n",
        "In this step, we set up the basic elements of our inventory management problem.\n",
        "\n",
        " - We define the **states**, which represent the different levels of inventory: Low, Medium, and High.\n",
        "\n",
        " - Actions are the decisions we can make regarding the inventory, such as ordering more products, maintaining the current inventory, or reducing the inventory.\n",
        "\n",
        " - Transition probabilities describe the likelihood of transitioning from one state to another when we take a specific action.\n",
        "\n",
        " - Rewards are assigned to each state-action pair to reflect the desirability of a particular action in a specific state.\n",
        "\n",
        " - Lastly, the discount factor is a value between 0 and 1 that determines the importance of immediate rewards compared to future rewards."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "OYmXoti6t31l"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file into a DataFrame\n",
        "# df = pd.read_csv('/content/drive/MyDrive/My Drive/My Courses/Business-Driven Deep Reinforcement Learning/Module 3 : Day 1 - Fundamentals/Day 1 - Fundamentals - Solutions/datasets/inventory_dataset.csv')\n",
        "\n",
        "# Step 1: Define the states, actions, transition probabilities, rewards, and discount factor\n",
        "states = [\"Low\", \"Medium\", \"High\"]\n",
        "\n",
        "actions = [\"Order\", \"Maintain\", \"Reduce\"]\n",
        "\n",
        "# We can get the transition prob from the previous examples (dataset) as well\n",
        "#transition_probs = {\n",
        "#    (\"Low\", \"Order\"): {\"Low\": 0.8, \"Medium\": 0.2, \"High\": 0.0},\n",
        "#    (\"Medium\", \"Order\"): {\"Low\": 0.1, \"Medium\": 0.8, \"High\": 0.1},\n",
        "#    (\"High\", \"Order\"): {\"Low\": 0.0, \"Medium\": 0.2, \"High\": 0.8},\n",
        "#    (\"Low\", \"Maintain\"): {\"Low\": 0.9, \"Medium\": 0.1, \"High\": 0.0},\n",
        "#    (\"Medium\", \"Maintain\"): {\"Low\": 0.0, \"Medium\": 0.9, \"High\": 0.1},\n",
        "#    (\"High\", \"Maintain\"): {\"Low\": 0.0, \"Medium\": 0.0, \"High\": 1.0},\n",
        "#    (\"Low\", \"Reduce\"): {\"Low\": 0.0, \"Medium\": 0.0, \"High\": 1.0},\n",
        "#    (\"Medium\", \"Reduce\"): {\"Low\": 0.0, \"Medium\": 0.0, \"High\": 1.0},\n",
        "#    (\"High\", \"Reduce\"): {\"Low\": 0.0, \"Medium\": 0.0, \"High\": 1.0}\n",
        "#}\n",
        "\n",
        "rewards = {\n",
        "    ('Low', 'Order'): 5,         # Reward for ordering more product when the inventory is already low\n",
        "    ('Medium', 'Order'): 0,      # 0 for ordering more product when the inventory is already medium\n",
        "    ('High', 'Order'): -1,       # Penalty for ordering more product when the inventory is already high\n",
        "    ('Low', 'Maintain'): 0.5,    # Slight reward for maintaining the current inventory when it is already low\n",
        "    ('Medium', 'Maintain'): 0.5, # Slight reward for maintaining the current inventory when it is already medium\n",
        "    ('High', 'Maintain'): -0.5,  # Slight penalty for maintaining the current inventory when it is already high\n",
        "    ('Low', 'Reduce'): -10,      # Higher penalty for reducing the inventory when it is low\n",
        "    ('Medium', 'Reduce'): 5,     # Lower reward for reducing the inventory when it is medium\n",
        "    ('High', 'Reduce'): 2        # Lower reward for reducing the inventory when it is high\n",
        "}\n",
        "\n",
        "discount_factor = 0.9\n",
        "\n",
        "# We can calculate the trans probs with the collected dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPNIvvt1dRSr"
      },
      "source": [
        "**Step 2**: Initialize the policy arbitrarily for each state\n",
        "\n",
        "  - The code initializes the policy dictionary, assigning an initial action\n",
        "\n",
        "  (initial_action) to each state in the states list. This serves as the starting policy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "YxSy_ArHgu2t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc1946cd-45e3-4044-8b02-e606cdafc3f8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Low': 'Reduce', 'Medium': 'Order', 'High': 'Order'}"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "# Step 2: Initialize the policy arbitrarily for each state\n",
        "policy = {state: random.choice(actions) for state in states}\n",
        "policy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q6: Please use policy iterarion and find the optimal policy"
      ],
      "metadata": {
        "id": "RNvJ1oOvZcKE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2PTneH3f9Se"
      },
      "source": [
        "**Step 3**: Perform policy iteration to find the optimal policy\n",
        "\n",
        "  - The code enters a loop that continues until the policy stabilizes, meaning no further changes occur in the policy.\n",
        "\n",
        "  - Step 3a: Policy Evaluation\n",
        "\n",
        "    - Within each iteration of the policy iteration loop, the code performs policy evaluation to estimate the value function values under the current policy.\n",
        "    - The values dictionary is initialized with zeros for each state.\n",
        "\n",
        "    - Step 3a.1: Iterative Value Update\n",
        "\n",
        "      - The code enters a nested loop that iterates until the value function values converges.\n",
        "      - For each state in states, the code calculates the expected value of the next state under the current policy.\n",
        "      - It does so by summing the products of the transition probabilities, rewards, and discounted values of the next states.\n",
        "      - The new value for the current state is stored in the new_value variable.\n",
        "\n",
        "    - Step 3a.2: Convergence Check\n",
        "\n",
        "      - The code tracks the maximum change in the value function (delta) by comparing the absolute difference between the new value and the old value for each state.\n",
        "      - If the maximum change (delta) is below a predefined threshold (epsilon), the loop breaks, indicating convergence.\n",
        "\n",
        "  - Step 3b: Policy Improvement\n",
        "\n",
        "      - After policy evaluation, the code proceeds to policy improvement to update the policy based on the current value function.\n",
        "\n",
        "    - Step 3b.1: Policy Update\n",
        "\n",
        "      - For each state in states, the code calculates the V-value for each action by considering the transition probabilities, rewards, and discounted values of the next states.\n",
        "      - It finds the action with the highest V-value and assigns it as the new action for the state in the policy dictionary.\n",
        "\n",
        "    - Step 3b.2: Policy Stability Check\n",
        "\n",
        "      - The code checks if the new action differs from the old action in any state.\n",
        "      - If the policy has changed for at least one state, the policy_stable flag is set to False, indicating that the policy is not yet stable.\n",
        "\n",
        "    - Step 3b.3: Convergence Check\n",
        "\n",
        "      - After policy improvement, the code checks if the policy has stabilized by evaluating the value of the policy_stable flag.\n",
        "      - If the policy remains stable (i.e., no changes occurred), the loop breaks, and the optimal policy has been found."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wxyVxcAQady"
      },
      "source": [
        "Initialization:\n",
        "Objective: Set initial conditions for the policy iteration loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "wdXONm-eQady"
      },
      "outputs": [],
      "source": [
        "epsilon = 0.0001  # Convergence threshold for the value function\n",
        "#epsilon: A small value representing the threshold below which if our value function changes, we consider it to have converged."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQ2sAjR3Qadz"
      },
      "source": [
        "Step 3a: Policy Evaluation:\n",
        "Objective: Given a policy, estimate the value function until it stabilizes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "W-JRxyGGQadz"
      },
      "outputs": [],
      "source": [
        "while True:\n",
        "#Objective: This outer loop will continue until the policy remains unchanged, indicating convergence to the optimal policy.\n",
        "\n",
        "    values = {state: 0 for state in states}  # Initialize the value function\n",
        "\n",
        "    while True:\n",
        "        delta = 0  # Track the maximum change across all states\n",
        "        for state in states:\n",
        "            action = policy[state]  # Get the action from the current policy\n",
        "\n",
        "            # Calculate the expected value for this state under the current policy's action\n",
        "            next_state_values = [\n",
        "                transition_probs[(state, action)][next_state] * (rewards[(state, action)] + discount_factor * values[next_state])\n",
        "                for next_state in states\n",
        "            ]\n",
        "            new_value = sum(next_state_values)\n",
        "\n",
        "            delta = max(delta, abs(new_value - values[state]))  # Update maximum change\n",
        "            values[state] = new_value  # Update the value function for the state\n",
        "\n",
        "        if delta < epsilon:  # Convergence check for the value function\n",
        "            break\n",
        "#This section repeatedly evaluates the value of states under the current policy until the change in values is less than the epsilon threshold\n",
        "\n",
        "#Step 3b: Policy Improvement: Objective: Adjust the policy based on the recently evaluated value function.\n",
        "\n",
        "    policy_stable = True  # Flag to check if the policy has changed\n",
        "\n",
        "    for state in states:\n",
        "        old_action = policy[state]  # Previous action from the current policy\n",
        "\n",
        "        # For each state, calculate the expected return for all possible actions\n",
        "        V_values = [\n",
        "            sum(\n",
        "                transition_probs[(state, action)][next_state] * (rewards[(state, action)] + discount_factor * values[next_state])\n",
        "                for next_state in states\n",
        "            )\n",
        "            for action in actions\n",
        "        ]\n",
        "\n",
        "        max_action_idx = V_values.index(max(V_values))  # Find the action with the highest expected return\n",
        "        new_action = actions[max_action_idx]  # Determine the new action\n",
        "\n",
        "        if old_action != new_action:  # If the policy changes for this state\n",
        "            policy_stable = False\n",
        "            policy[state] = new_action  # Update the policy for this state\n",
        "\n",
        "    if policy_stable:  # If the policy didn't change for any state, we've found the optimal policy\n",
        "        break\n",
        "#This section improves the policy by selecting the best action for each state based on the current value function.\n",
        "#If the policy remains unchanged after this step, it is considered optimal, and the process terminates."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbRqH3nHgbEv"
      },
      "source": [
        "**Step 4**: Print the final value function and optimal policy\n",
        "\n",
        "  - Finally, the code prints the final value function (values) and the optimal policy (policy)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "jNqDVlbBgy3B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ab53301-f0c3-43d0-f2bc-a3e7fe5802f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Value Function:\n",
            "{'Low': 44.66741981719094, 'Medium': 44.58091696596418, 'High': 41.76390829469604}\n",
            "\n",
            "Optimal Policy:\n",
            "{'Low': 'Order', 'Medium': 'Reduce', 'High': 'Reduce'}\n"
          ]
        }
      ],
      "source": [
        "# Step 4: Print the final value function and optimal policy\n",
        "print(\"Final Value Function:\")\n",
        "print(values)\n",
        "print(\"\\nOptimal Policy:\")\n",
        "print(policy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgF93sz7Qad3"
      },
      "source": [
        "# Summary and Takeaways:\n",
        "In this lab, we deepened our exploration into iterative algorithms for decision-making in MDPs. We learned about the importance of convergence and how the Bellman Equation plays a pivotal role."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szcrQXJwFvHE"
      },
      "source": [
        "If you have any questions, please contact me at salihtutun@wustl.edu\n",
        "\n",
        "by Salih Tutun"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}